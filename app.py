"""Main Chainlit application for AI Agent with Model Invocation."""

import asyncio
import logging
import os
from typing import Optional

import chainlit as cl
from chainlit.input_widget import Switch, Select
from dotenv import load_dotenv

from src.config.model_config import (
    ModelProvider,
    get_model_config,
    get_available_providers,
)
from src.config.search_config import get_search_config, is_search_available
from src.config.agent_config import (
    get_agent_config, 
    get_default_mode,
    create_agent_llms_from_config,
)
from src.config.langsmith_config import is_langsmith_enabled, get_langsmith_config
from src.models.factory import get_model_wrapper
from src.prompts.templates import (
    DEFAULT_SYSTEM_MESSAGE,
    count_prompt_tokens,
    build_system_message_with_search,
)
from src.search.search_service import SearchService
from src.search.citation_processor import CitationProcessor
from src.agents import ReActAgent, AgentStep
from src.agents.tools import create_search_tool

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=os.getenv("LOG_LEVEL", "INFO"),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# Global caches (initialized once at first session start)
_global_search_service: Optional[SearchService] = None
_search_service_initialized = False
_search_initialization_lock = asyncio.Lock()

_global_model_wrappers = {}  # Cache for model wrappers by provider
_model_initialization_lock = asyncio.Lock()


async def get_or_create_model_wrapper(provider: str):
    """Get cached model wrapper or create new one.
    
    Model wrappers are cached per provider to avoid repeated initialization.
    """
    global _global_model_wrappers
    
    # Return cached wrapper if exists
    if provider in _global_model_wrappers:
        logger.debug(f"Reusing cached model wrapper for provider: {provider}")
        return _global_model_wrappers[provider]
    
    async with _model_initialization_lock:
        # Double-check after acquiring lock
        if provider in _global_model_wrappers:
            return _global_model_wrappers[provider]
        
        # Create new wrapper
        logger.info(f"ğŸ¤– Initializing model wrapper for provider: {provider} (one-time setup)...")
        model_wrapper = get_model_wrapper(provider=provider)
        _global_model_wrappers[provider] = model_wrapper
        logger.info(f"âœ… Model wrapper initialized: {model_wrapper.config.model_name}")
        
        return model_wrapper


async def initialize_search_service() -> Optional[SearchService]:
    """Initialize search service once and cache the result.
    
    This function is called only once when the first user session starts.
    Subsequent sessions will reuse the cached search service.
    """
    global _global_search_service, _search_service_initialized
    
    # Double-check pattern with lock
    if _search_service_initialized:
        return _global_search_service
    
    async with _search_initialization_lock:
        # Check again after acquiring lock
        if _search_service_initialized:
            return _global_search_service
        
        search_available = is_search_available()
        if not search_available:
            _search_service_initialized = True
            return None
        
        try:
            logger.info("ğŸ” Initializing search service (one-time setup)...")
            search_config = get_search_config()
            search_service = SearchService(
                searxng_url=search_config.searxng_url,
                timeout=search_config.timeout,
                max_results=search_config.max_results,
                max_content_length=search_config.max_content_length,
            )
            
            # Perform one-time health check
            health_ok = await search_service.client.health_check()
            
            if health_ok:
                logger.info("âœ… Search service initialized successfully")
                _global_search_service = search_service
            else:
                logger.warning(
                    "âš ï¸ SearXNG health check failed. Search will be unavailable.\n"
                    "ğŸ“– Deployment guide: docs/guides/searxng-deployment.md"
                )
                _global_search_service = None
        
        except Exception as e:
            logger.error(
                f"âŒ Failed to initialize search service: {str(e)}\n"
                "Search functionality will be unavailable.\n"
                "To enable search:\n"
                "  1. Deploy SearXNG locally (see docs/guides/searxng-deployment.md)\n"
                "  2. Ensure SEARXNG_URL in .env points to your instance\n"
                "  3. Run verification: bash openspec/changes/update-searxng-local-deployment/verify-searxng.sh"
            )
            _global_search_service = None
        
        _search_service_initialized = True
        return _global_search_service


@cl.on_settings_update
async def settings_update(settings):
    """Handle chat settings updates."""
    try:
        # Handle conversation mode switch
        conversation_mode = settings.get("conversation_mode")
        current_mode = cl.user_session.get("conversation_mode", "chat")
        
        if conversation_mode and conversation_mode != current_mode:
            # Mode switched - reset conversation
            cl.user_session.set("conversation_mode", conversation_mode)
            cl.user_session.set("conversation_history", [])
            
            # Re-initialize agent if switching to agent mode
            if conversation_mode == "agent":
                model_wrapper = cl.user_session.get("model_wrapper")
                search_service = cl.user_session.get("search_service")
                
                if model_wrapper and search_service:
                    # Create agent
                    search_tool = create_search_tool(search_service)
                    agent_config = get_agent_config()
                    
                    # Create LLMs from config (supports dual LLM mode)
                    current_provider = cl.user_session.get("current_provider", "openai")
                    function_call_llm, answer_llm = create_agent_llms_from_config(
                        default_provider=current_provider,
                        agent_config=agent_config
                    )
                    
                    agent = ReActAgent(
                        llm=function_call_llm,
                        search_tool=search_tool,
                        config=agent_config,
                        answer_llm=answer_llm,
                    )
                    cl.user_session.set("agent", agent)
                    
                    mode_display = "ğŸ¤– Agent æ¨¡å¼"
                    mode_desc = "æ¨¡å‹ä¼šè‡ªä¸»å†³ç­–ä½•æ—¶ä½¿ç”¨æœç´¢å·¥å…·"
                else:
                    mode_display = "ğŸ¤– Agent æ¨¡å¼ï¼ˆæœç´¢ä¸å¯ç”¨ï¼‰"
                    mode_desc = "æœç´¢æœåŠ¡æœªå¯ç”¨ï¼ŒAgent åŠŸèƒ½å—é™"
            else:
                mode_display = "ğŸ’¬ Chat æ¨¡å¼"
                mode_desc = "å¸¸è§„å¯¹è¯ï¼Œå¯æ‰‹åŠ¨æ§åˆ¶æœç´¢"
            
            await cl.Message(
                content=f"âœ… å·²åˆ‡æ¢åˆ° {mode_display}\n\n{mode_desc}\n\nå¯¹è¯å†å²å·²æ¸…é™¤ã€‚",
                author="System",
            ).send()
        
        # Handle web search toggle (only in Chat mode)
        search_enabled = settings.get("web_search", False)
        search_service = cl.user_session.get("search_service")
        current_mode = cl.user_session.get("conversation_mode", "chat")
        
        if current_mode == "chat" and search_service:
            # Update search enabled state
            cl.user_session.set("search_enabled", search_enabled)
            
            # Send confirmation message
            status = "âœ… å·²å¯ç”¨" if search_enabled else "âŒ å·²ç¦ç”¨"
            await cl.Message(
                content=f"è”ç½‘æœç´¢ {status}",
                author="System",
            ).send()
        elif current_mode == "agent" and "web_search" in settings:
            # In Agent mode, search is always controlled by the agent
            await cl.Message(
                content="â„¹ï¸ Agent æ¨¡å¼ä¸‹ï¼Œæœç´¢ç”±æ¨¡å‹è‡ªä¸»å†³ç­–ï¼Œæ— éœ€æ‰‹åŠ¨æ§åˆ¶ã€‚",
                author="System",
            ).send()
        
        # Handle DeepSeek model variant selection
        deepseek_model = settings.get("deepseek_model")
        current_provider = cl.user_session.get("current_provider")
        
        if deepseek_model and current_provider == "deepseek":
            # Update model variant
            model_wrapper = cl.user_session.get("model_wrapper")
            if model_wrapper:
                # Update model name and variant
                model_wrapper.config.model_name = deepseek_model
                model_wrapper.config.model_variant = deepseek_model
                
                # Update max_tokens based on model variant
                # deepseek-chat: max 8K, deepseek-reasoner: max 64K
                if deepseek_model == "deepseek-reasoner":
                    # Ensure max_tokens is within reasoner's limit (64K)
                    if model_wrapper.config.max_tokens > 65536:
                        model_wrapper.config.max_tokens = 65536
                else:
                    # Ensure max_tokens is within chat's limit (8K)
                    if model_wrapper.config.max_tokens > 8192:
                        model_wrapper.config.max_tokens = 8192
                
                # Clear conversation history
                cl.user_session.set("conversation_history", [])
                
                # Send confirmation message
                model_display = "ğŸ’­ æ¨ç†æ¨¡å‹" if deepseek_model == "deepseek-reasoner" else "ğŸ’¬ å¯¹è¯æ¨¡å‹"
                max_output = "64K" if deepseek_model == "deepseek-reasoner" else "8K"
                await cl.Message(
                    content=f"âœ… å·²åˆ‡æ¢åˆ° DeepSeek {model_display}\n\næ¨¡å‹: {deepseek_model}\næœ€å¤§è¾“å‡º: {max_output}\nå¯¹è¯å†å²å·²æ¸…é™¤ã€‚",
                    author="System",
                ).send()
    
    except Exception as e:
        logger.error(f"Error updating settings: {str(e)}")
        await cl.Message(
            content=f"âŒ è®¾ç½®æ›´æ–°å¤±è´¥: {str(e)}",
            author="System",
        ).send()


@cl.on_chat_start
async def start():
    """Initialize chat session."""
    try:
        # Initialize LangSmith monitoring (if configured)
        langsmith_enabled = is_langsmith_enabled()
        if langsmith_enabled:
            langsmith_config = get_langsmith_config()
            logger.info(
                f"ğŸ“Š LangSmith ç›‘æ§å·²å¯ç”¨ (é¡¹ç›®: {langsmith_config.project})"
            )
        else:
            logger.debug("LangSmith ç›‘æ§æœªå¯ç”¨ï¼ˆæœªé…ç½® API å¯†é’¥ï¼‰")
        
        # Get available providers
        available_providers = get_available_providers()
        
        if not available_providers:
            await cl.Message(
                content="âš ï¸ No model providers configured. Please set up API keys in .env file.",
                author="System",
            ).send()
            return
        
        # Get default provider
        default_provider = os.getenv("DEFAULT_PROVIDER", "openai")
        if default_provider not in available_providers:
            default_provider = available_providers[0]
        
        # Get cached model wrapper (initialized once per provider)
        try:
            model_wrapper = await get_or_create_model_wrapper(provider=default_provider)
            config = model_wrapper.config
            
            # Get cached search service (initialized once on first session)
            search_service = await initialize_search_service()
            search_available = search_service is not None
            
            # Get default conversation mode
            default_mode = get_default_mode()
            
            # Initialize agent if in agent mode and search is available
            agent = None
            if default_mode == "agent" and search_service:
                try:
                    logger.info("ğŸ¤– Initializing Agent mode...")
                    search_tool = create_search_tool(search_service)
                    agent_config = get_agent_config()
                    
                    # Create LLMs from config (supports dual LLM mode)
                    function_call_llm, answer_llm = create_agent_llms_from_config(
                        default_provider=default_provider,
                        agent_config=agent_config
                    )
                    
                    agent = ReActAgent(
                        llm=function_call_llm,
                        search_tool=search_tool,
                        config=agent_config,
                        answer_llm=answer_llm,
                    )
                    logger.info("âœ… Agent initialized successfully")
                except Exception as e:
                    logger.error(f"âŒ Failed to initialize Agent: {e}")
                    default_mode = "chat"  # Fallback to chat mode
            
            # Store in session
            cl.user_session.set("model_wrapper", model_wrapper)
            cl.user_session.set("current_provider", default_provider)
            cl.user_session.set("available_providers", available_providers)
            cl.user_session.set("conversation_history", [])
            cl.user_session.set("search_service", search_service)
            # Get default search enabled state from config (Chat mode only, Agent mode controls search automatically)
            search_config = get_search_config()
            default_search_enabled = search_config.enabled if default_mode == "chat" else False
            cl.user_session.set("search_enabled", default_search_enabled)
            cl.user_session.set("conversation_mode", default_mode)
            cl.user_session.set("agent", agent)
            
            # Prepare welcome message first
            if search_available and search_service:
                search_status = "âœ… å¯ç”¨ (æœ¬åœ°éƒ¨ç½²)"
                search_hint = ""
            else:
                search_status = "âŒ ä¸å¯ç”¨"
                search_hint = "\n\nğŸ’¡ **å¯ç”¨è”ç½‘æœç´¢:**\n1. éƒ¨ç½² SearXNG: `docs/guides/searxng-deployment.md`\n2. é…ç½® `.env`: `SEARXNG_URL=http://localhost:8080`\n3. é‡å¯åº”ç”¨\n"
            
            # Build model info
            model_info = f"**Current Model:** {config.model_name}"
            if default_provider == "deepseek" and config.model_variant:
                model_type = "ğŸ’­ æ¨ç†æ¨¡å‹" if config.model_variant == "deepseek-reasoner" else "ğŸ’¬ å¯¹è¯æ¨¡å‹"
                model_info += f" ({model_type})"
            
            # Build conversation mode info
            mode_emoji = "ğŸ¤–" if default_mode == "agent" else "ğŸ’¬"
            mode_name = "Agent æ¨¡å¼" if default_mode == "agent" else "Chat æ¨¡å¼"
            if default_mode == "agent":
                if search_available:
                    mode_desc = "æ¨¡å‹ä¼šè‡ªä¸»å†³ç­–ä½•æ—¶ä½¿ç”¨æœç´¢å·¥å…·"
                else:
                    mode_desc = "æœç´¢æœåŠ¡æœªå¯ç”¨ï¼ŒAgent åŠŸèƒ½å—é™"
            else:
                mode_desc = "å¸¸è§„å¯¹è¯ï¼Œå¯æ‰‹åŠ¨æ§åˆ¶æœç´¢"
            
            # Build UI settings hint
            ui_settings_hint = f"""**ğŸ’¡ ä½¿ç”¨ UI è®¾ç½®é¢æ¿:**
- ç‚¹å‡»å³ä¸Šè§’ âš™ï¸ å›¾æ ‡æ‰“å¼€è®¾ç½®é¢æ¿
- é€‰æ‹© \"ğŸ”€ å¯¹è¯æ¨¡å¼\" åˆ‡æ¢ Chat/Agent æ¨¡å¼
- åœ¨ Chat æ¨¡å¼ä¸‹å¯åˆ‡æ¢ "ğŸ” è”ç½‘æœç´¢" å¼€å…³"""
            
            if default_provider == "deepseek":
                ui_settings_hint += "\n- é€‰æ‹© \"ğŸ¤– DeepSeek æ¨¡å‹\" å¯åˆ‡æ¢å¯¹è¯/æ¨ç†æ¨¡å‹"
            
            ui_settings_hint += "\n- è®¾ç½®ä¼šç«‹å³ç”Ÿæ•ˆ"
            
            welcome_msg = f"""# ğŸ¤– Welcome to AI Agent Chat!

{model_info}
**Provider:** {config.provider}
**Temperature:** {config.temperature}
**Max Tokens:** {config.max_tokens}

**Available Providers:** {', '.join(available_providers)}
**è”ç½‘æœç´¢:** {search_status}{search_hint}

**å¯¹è¯æ¨¡å¼:** {mode_emoji} {mode_name} - {mode_desc}

You can start chatting now! Type your message below.

{ui_settings_hint}

**Commands (å¤‡ç”¨æ–¹å¼):**
- `/switch <provider>` - Switch to a different model provider
- `/mode <chat|agent>` - Switch conversation mode
- `/search <on|off>` - Enable or disable web search (Chat mode only)
- `/config` - View current configuration
- `/reset` - Clear conversation history
- `/help` - Show this help message
"""
            
            # Send welcome message and chat settings simultaneously
            # This prevents showing a blank screen before content appears
            welcome_message = cl.Message(
                content=welcome_msg,
                author="System",
            )
            
            # Build settings widgets
            settings_widgets = [
                Select(
                    id="conversation_mode",
                    label="ğŸ”€ å¯¹è¯æ¨¡å¼",
                    values=["chat", "agent"],
                    initial_value=default_mode,
                    description="Chat: å¸¸è§„å¯¹è¯ | Agent: è‡ªä¸»å†³ç­–æœç´¢",
                ),
                Switch(
                    id="web_search",
                    label="ğŸ” è”ç½‘æœç´¢ (Chatæ¨¡å¼)",
                    initial=default_search_enabled,
                    description="ä»…åœ¨ Chat æ¨¡å¼ä¸‹æœ‰æ•ˆï¼ŒAgent æ¨¡å¼ç”±æ¨¡å‹è‡ªä¸»å†³ç­–",
                    disabled=(search_service is None),
                )
            ]
            
            # Add DeepSeek model selection if using DeepSeek provider
            if default_provider == "deepseek":
                current_variant = config.model_variant or "deepseek-chat"
                settings_widgets.append(
                    Select(
                        id="deepseek_model",
                        label="ğŸ¤– DeepSeek æ¨¡å‹",
                        values=["deepseek-chat", "deepseek-reasoner"],
                        initial_value=current_variant,
                        description="é€‰æ‹© DeepSeek æ¨¡å‹ç±»å‹",
                    )
                )
            
            chat_settings = cl.ChatSettings(settings_widgets)
            
            # Send both at the same time to avoid double window flash
            await asyncio.gather(
                welcome_message.send(),
                chat_settings.send()
            )
            
        except Exception as e:
            logger.error(f"Failed to initialize model: {str(e)}")
            await cl.Message(
                content=f"âŒ Error initializing model: {str(e)}\n\nPlease check your .env configuration.",
                author="System",
            ).send()
    
    except Exception as e:
        logger.error(f"Error in chat start: {str(e)}")
        await cl.Message(
            content=f"âŒ Initialization error: {str(e)}",
            author="System",
        ).send()




@cl.on_message
async def main(message: cl.Message):
    """Handle incoming messages."""
    try:
        user_message = message.content.strip()
        
        # Handle commands
        if user_message.startswith("/"):
            await handle_command(user_message)
            return
        
        # Get model wrapper from session
        model_wrapper = cl.user_session.get("model_wrapper")
        if not model_wrapper:
            await cl.Message(
                content="âš ï¸ Model not initialized. Please restart the chat.",
                author="System",
            ).send()
            return
        
        # Check conversation mode
        conversation_mode = cl.user_session.get("conversation_mode", "chat")
        
        # Route to appropriate handler
        if conversation_mode == "agent":
            await handle_agent_mode(user_message)
        else:
            await handle_chat_mode(user_message)
    
    except Exception as e:
        logger.error(f"Error in message handler: {str(e)}")
        await cl.Message(
            content=f"âŒ Error: {str(e)}",
            author="System",
        ).send()


async def handle_agent_mode(user_message: str):
    """Handle Agent mode conversation.
    
    Args:
        user_message: User's input message
    """
    try:
        agent = cl.user_session.get("agent")
        if not agent:
            await cl.Message(
                content="âš ï¸ Agent æœªåˆå§‹åŒ–ã€‚è¯·åˆ‡æ¢åˆ° Chat æ¨¡å¼æˆ–é‡å¯ä¼šè¯ã€‚",
                author="System",
            ).send()
            return
        
        logger.info(f"ğŸ¤– Agent æ¨¡å¼å¤„ç†: {user_message}")
        
        # Track steps for better UI handling
        thinking_step = None
        current_action_step = None
        
        # Stream agent execution with timeout
        try:
            async for step in agent.stream(user_message):
                logger.debug(f"æ”¶åˆ° Agent æ­¥éª¤: type={step.type}, content_length={len(step.content) if step.content else 0}")
                
                if step.type == "reasoning":
                    # Show thinking process in collapsible step
                    if thinking_step is None:
                        thinking_step = cl.Step(name="ğŸ’­ æ€è€ƒä¸­", type="tool")
                        await thinking_step.__aenter__()
                    thinking_step.output = step.content
                
                elif step.type == "action":
                    # Close thinking step if open
                    if thinking_step:
                        await thinking_step.__aexit__(None, None, None)
                        thinking_step = None
                    
                    # Show tool call
                    tool_name = step.metadata.get("tool", "unknown") if step.metadata else "unknown"
                    tool_input = step.metadata.get("tool_input", "") if step.metadata else ""
                    current_action_step = cl.Step(name=f"ğŸ› ï¸ ä½¿ç”¨å·¥å…·: {tool_name}", type="tool")
                    await current_action_step.__aenter__()
                    current_action_step.output = f"è¾“å…¥å‚æ•°: {tool_input}"
                
                elif step.type == "observation":
                    # Close action step if open
                    if current_action_step:
                        await current_action_step.__aexit__(None, None, None)
                        current_action_step = None
                    
                    # Show tool result
                    async with cl.Step(name="ğŸ’¡ å·¥å…·ç»“æœ", type="tool") as observation_step:
                        observation_step.output = step.content
                
                elif step.type == "final":
                    # Close any open steps
                    if thinking_step:
                        await thinking_step.__aexit__(None, None, None)
                        thinking_step = None
                    if current_action_step:
                        await current_action_step.__aexit__(None, None, None)
                        current_action_step = None
                    
                    # Show final answer
                    final_msg = cl.Message(
                        content=step.content,
                        author="Assistant",
                    )
                    await final_msg.send()
                    
                    # Update conversation history
                    history = cl.user_session.get("conversation_history", [])
                    history.append({
                        "role": "user",
                        "content": user_message,
                    })
                    history.append({
                        "role": "assistant",
                        "content": step.content,
                    })
                    cl.user_session.set("conversation_history", history)
                    break  # Exit loop after final answer
                
                elif step.type == "error":
                    # Close any open steps
                    if thinking_step:
                        await thinking_step.__aexit__(None, None, None)
                        thinking_step = None
                    if current_action_step:
                        await current_action_step.__aexit__(None, None, None)
                        current_action_step = None
                    
                    # Show error
                    await cl.Message(
                        content=f"âŒ Agent æ‰§è¡Œé”™è¯¯: {step.content}",
                        author="System",
                    ).send()
                    break  # Exit loop on error
            
            # Ensure all steps are closed
            if thinking_step:
                try:
                    await thinking_step.__aexit__(None, None, None)
                except:
                    pass
            if current_action_step:
                try:
                    await current_action_step.__aexit__(None, None, None)
                except:
                    pass
            
            logger.info("âœ… Agent æ¨¡å¼å¤„ç†å®Œæˆ")
        
        except asyncio.TimeoutError:
            logger.error("â±ï¸ Agent æ‰§è¡Œè¶…æ—¶")
            await cl.Message(
                content="â±ï¸ Agent æ‰§è¡Œè¶…æ—¶ï¼Œè¯·å°è¯•ç®€åŒ–é—®é¢˜æˆ–åˆ‡æ¢åˆ° Chat æ¨¡å¼ã€‚",
                author="System",
            ).send()
        except Exception as stream_error:
            logger.error(f"âŒ Agent æµå¼å¤„ç†é”™è¯¯: {stream_error}", exc_info=True)
            await cl.Message(
                content=f"âŒ Agent æ‰§è¡Œå¤±è´¥: {str(stream_error)}\n\nè¯·å°è¯•åˆ‡æ¢åˆ° Chat æ¨¡å¼ã€‚",
                author="System",
            ).send()
    
    except Exception as e:
        logger.error(f"âŒ Agent æ¨¡å¼é”™è¯¯: {str(e)}", exc_info=True)
        await cl.Message(
            content=f"âŒ Agent æ‰§è¡Œå¤±è´¥: {str(e)}\n\nè¯·å°è¯•åˆ‡æ¢åˆ° Chat æ¨¡å¼ã€‚",
            author="System",
        ).send()


async def handle_chat_mode(user_message: str):
    """Handle Chat mode conversation.
    
    Args:
        user_message: User's input message
    """
    try:
        # Get model wrapper from session
        model_wrapper = cl.user_session.get("model_wrapper")
        if not model_wrapper:
            await cl.Message(
                content="âš ï¸ Model not initialized. Please restart the chat.",
                author="System",
            ).send()
            return
        
        # Check if search is enabled
        search_enabled = cl.user_session.get("search_enabled", False)
        search_service = cl.user_session.get("search_service")
        
        # Perform search if enabled
        search_response = None
        search_results_text = None
        if search_enabled and search_service:
            try:
                # Show searching indicator
                search_msg = cl.Message(
                    content="ğŸ” æ­£åœ¨æœç´¢ç›¸å…³ä¿¡æ¯...",
                    author="System",
                )
                await search_msg.send()
                
                # Perform search
                search_response = await search_service.search(user_message)
                
                # Update search message
                if search_response and not search_response.is_empty():
                    search_msg.content = f"âœ… æ‰¾åˆ° {search_response.total_results} æ¡æœç´¢ç»“æœ"
                    await search_msg.update()
                    
                    # Format search results for prompt
                    search_results_text = search_service.format_for_prompt(search_response)
                else:
                    search_msg.content = "âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœï¼Œå°†åŸºäºæ¨¡å‹çŸ¥è¯†å›ç­”"
                    await search_msg.update()
            
            except Exception as e:
                logger.error(f"Search error: {str(e)}")
                await cl.Message(
                    content=f"âš ï¸ æœç´¢å¤±è´¥: {str(e)}\n\nå°†ç»§ç»­ä½¿ç”¨æ¨¡å‹çŸ¥è¯†å›ç­”ã€‚",
                    author="System",
                ).send()
        
        try:
            # Build system message with search results if available
            system_message = build_system_message_with_search(
                DEFAULT_SYSTEM_MESSAGE,
                search_results_text,
            )
            
            # Count tokens
            token_count = count_prompt_tokens(
                user_message,
                system_message,
            )
            
            logger.info(f"Processing message with {token_count} tokens (search: {search_enabled})")
            
            # Check if using deepseek-reasoner
            config = model_wrapper.config
            is_reasoner = (
                config.provider == "deepseek" and 
                config.model_variant == "deepseek-reasoner"
            )
            
            # Generate streaming response
            thinking_step = None
            reasoning_content = ""
            full_response = ""
            response_msg = None
            
            try:
                async for chunk in model_wrapper.generate_stream(
                    prompt=user_message,
                    system_message=system_message,
                ):
                    # Skip if chunk is None or missing required attributes
                    if chunk is None:
                        logger.warning("Received None chunk from model")
                        continue
                    if not hasattr(chunk, 'content') or chunk.content is None:
                        logger.warning(f"Chunk missing content: {chunk}")
                        continue
                    if not hasattr(chunk, 'chunk_type'):
                        logger.warning(f"Chunk missing chunk_type: {chunk}")
                        continue
                    
                    # Handle reasoning content (only for deepseek-reasoner)
                    if chunk.chunk_type == "reasoning":
                        if thinking_step is None:
                            # Create thinking message for real-time expanded view
                            logger.info("ğŸ’­ Creating thinking message (expanded for streaming)")
                            thinking_step = cl.Message(
                                content="",
                                author="ğŸ’­ æ€è€ƒä¸­",
                            )
                            await thinking_step.send()
                            logger.debug("Thinking message created for real-time streaming")
                        
                        # Stream reasoning content in real-time
                        reasoning_content += chunk.content
                        thinking_step.content = reasoning_content
                        await thinking_step.update()
                    
                    # Handle answer content
                    elif chunk.chunk_type == "answer":
                        # Convert thinking message to collapsible step on first answer chunk
                        if thinking_step is not None:
                            if not hasattr(thinking_step, '_collapsed_already'):
                                logger.info("ğŸ’¡ Converting thinking to collapsed step (answer started)")
                                
                                # Create a collapsed Step with the thinking content
                                collapsed_step = cl.Step(name="ğŸ’¡ æ€è€ƒè¿‡ç¨‹", type="tool")
                                collapsed_step.output = reasoning_content
                                
                                # Use context manager to create collapsed step
                                async with collapsed_step:
                                    pass  # Step automatically closes after context
                                
                                # Remove the expanded thinking message
                                thinking_step.content = ""
                                await thinking_step.remove()
                                
                                # Mark as collapsed to ensure idempotency
                                thinking_step._collapsed_already = True
                                logger.debug("Thinking converted to collapsed step successfully")
                        
                        # Create response message if not exists
                        if response_msg is None:
                            response_msg = cl.Message(
                                content="",
                                author="Assistant",
                            )
                            await response_msg.send()
                        
                        # Stream answer content in real-time
                        full_response += chunk.content
                        response_msg.content = full_response
                        await response_msg.update()
            
            finally:
                # Ensure thinking message is converted to collapsed step even in case of errors
                if thinking_step is not None and not hasattr(thinking_step, '_collapsed_already'):
                    logger.info("ğŸ’¡ Converting thinking to collapsed step (cleanup)")
                    try:
                        # Create a collapsed Step with the thinking content
                        collapsed_step = cl.Step(name="ğŸ’¡ æ€è€ƒè¿‡ç¨‹", type="tool")
                        collapsed_step.output = reasoning_content
                        
                        # Use context manager to create collapsed step
                        async with collapsed_step:
                            pass  # Step automatically closes after context
                        
                        # Remove the expanded thinking message
                        if hasattr(thinking_step, 'remove'):
                            thinking_step.content = ""
                            await thinking_step.remove()
                        
                        logger.debug("Thinking cleanup completed")
                    except Exception as cleanup_error:
                        logger.error(f"Error during thinking step cleanup: {cleanup_error}")
            
            # Process inline citations if search was used
            if search_response and not search_response.is_empty() and response_msg:
                try:
                    logger.info("ğŸ”— Processing inline citations")
                    citation_processor = CitationProcessor(search_response)
                    processed_response = citation_processor.process_response(full_response)
                    
                    # Update the response message with clickable citations
                    response_msg.content = processed_response
                    await response_msg.update()
                    
                    logger.info("âœ… Inline citations processed successfully")
                except Exception as e:
                    logger.error(f"Failed to process citations: {e}")
                    # Continue without citations on error
            
            # Display search sources if available
            if search_response and not search_response.is_empty():
                sources_text = search_service.format_sources(search_response)
                await cl.Message(
                    content=sources_text,
                    author="System",
                ).send()
            
            # Update conversation history
            history = cl.user_session.get("conversation_history", [])
            history.append({
                "role": "user",
                "content": user_message,
            })
            history.append({
                "role": "assistant",
                "content": full_response,
            })
            cl.user_session.set("conversation_history", history)
            
            # Count completion tokens (approximate)
            completion_tokens = model_wrapper.count_tokens(full_response)
            total_tokens = token_count + completion_tokens
            
            # Send metadata as a separate message
            search_info = f"\n- Search: {'Enabled âœ…' if search_enabled else 'Disabled'}"
            if search_enabled and search_response:
                search_info += f" ({search_response.total_results} results)"
            
            metadata_msg = f"""
---
**ğŸ“Š Response Metadata:**
- Model: {model_wrapper.config.model_name}
- Tokens Used: ~{total_tokens} (prompt: ~{token_count}, completion: ~{completion_tokens}){search_info}
"""
            
            await cl.Message(
                content=metadata_msg,
                author="System",
            ).send()
        
        except Exception as e:
            logger.error(f"Error generating response: {str(e)}")
            if response_msg is not None:
                response_msg.content = f"âŒ Error generating response: {str(e)}\n\nPlease try again."
                await response_msg.update()
            else:
                await cl.Message(
                    content=f"âŒ Error generating response: {str(e)}\n\nPlease try again.",
                    author="Assistant",
                ).send()
    
    except Exception as e:
        logger.error(f"Error in message handler: {str(e)}")
        await cl.Message(
            content=f"âŒ Error: {str(e)}",
            author="System",
        ).send()


async def handle_command(command: str):
    """Handle slash commands.
    
    Args:
        command: Command string (e.g., '/switch openai')
    """
    parts = command.split(maxsplit=1)
    cmd = parts[0].lower()
    args = parts[1] if len(parts) > 1 else ""
    
    if cmd == "/help":
        current_provider = cl.user_session.get("current_provider", "")
        
        # Build UI settings hint
        ui_hint = """**ğŸ’¡ æ¨èä½¿ç”¨ UI è®¾ç½®é¢æ¿:**
- ç‚¹å‡»å³ä¸Šè§’ âš™ï¸ å›¾æ ‡æ‰“å¼€è®¾ç½®é¢æ¿
- ç›´æ¥åˆ‡æ¢ "ğŸ” è”ç½‘æœç´¢" å¼€å…³"""
        
        if current_provider == "deepseek":
            ui_hint += "\n- é€‰æ‹© \"ğŸ¤– DeepSeek æ¨¡å‹\" å¯åˆ‡æ¢å¯¹è¯/æ¨ç†æ¨¡å‹"
        
        help_msg = f"""# ğŸ“– Available Commands

{ui_hint}

**å‘½ä»¤åˆ—è¡¨ (å¤‡ç”¨æ–¹å¼):**
- `/switch <provider>` - Switch to a different model provider (openai, anthropic, deepseek)
- `/mode <chat|agent>` - Switch conversation mode (æ¨èä½¿ç”¨UIåˆ‡æ¢)
- `/search <on|off>` - Enable or disable web search (ä»…Chatæ¨¡å¼ï¼Œæ¨èä½¿ç”¨UIå¼€å…³)
- `/config` - View current configuration
- `/reset` - Clear conversation history
- `/help` - Show this help message

**Examples:**
```
/switch deepseek
/mode agent
/mode chat
/search on
/search off
```

**ğŸ”€ å¯¹è¯æ¨¡å¼:**
- **Chat æ¨¡å¼** (ğŸ’¬): å¸¸è§„å¯¹è¯ï¼Œå¯æ‰‹åŠ¨æ§åˆ¶è”ç½‘æœç´¢
- **Agent æ¨¡å¼** (ğŸ¤–): æ¨¡å‹è‡ªä¸»å†³ç­–ä½•æ—¶ä½¿ç”¨æœç´¢å·¥å…·ï¼Œå®ç° ReAct å¾ªç¯

**ğŸ’­ DeepSeek Reasoner æ¨¡å‹:**
- é€‰æ‹© deepseek-reasoner åï¼Œæ¨¡å‹ä¼šå…ˆå±•ç¤ºæ€è€ƒè¿‡ç¨‹
- æ€è€ƒå†…å®¹ä¼šåœ¨å¼€å§‹å›ç­”æ—¶è‡ªåŠ¨æŠ˜å 
- å¯ç‚¹å‡» "ğŸ’¡ æ€è€ƒè¿‡ç¨‹" å±•å¼€æŸ¥çœ‹
"""
        await cl.Message(content=help_msg, author="System").send()
    
    elif cmd == "/config":
        model_wrapper = cl.user_session.get("model_wrapper")
        if not model_wrapper:
            await cl.Message(
                content="âš ï¸ No model configured.",
                author="System",
            ).send()
            return
        
        config = model_wrapper.config
        search_enabled = cl.user_session.get("search_enabled", False)
        search_service = cl.user_session.get("search_service")
        conversation_mode = cl.user_session.get("conversation_mode", "chat")
        
        search_status = "âœ… Enabled" if search_enabled else "âŒ Disabled"
        if not search_service:
            search_status = "âš ï¸ Not Available"
        
        mode_emoji = "ğŸ¤–" if conversation_mode == "agent" else "ğŸ’¬"
        mode_name = "Agent æ¨¡å¼" if conversation_mode == "agent" else "Chat æ¨¡å¼"
        
        # Build model info
        model_info = f"**Model:** {config.model_name}"
        if config.provider == "deepseek" and config.model_variant:
            model_type = "ğŸ’­ æ¨ç†æ¨¡å‹" if config.model_variant == "deepseek-reasoner" else "ğŸ’¬ å¯¹è¯æ¨¡å‹"
            model_info += f"\n**Model Type:** {model_type}"
        
        # Build UI hints
        ui_hints = "ğŸ’¡ **æç¤º:** å¯é€šè¿‡å³ä¸Šè§’ âš™ï¸ è®¾ç½®é¢æ¿åˆ‡æ¢å¯¹è¯æ¨¡å¼å’Œè”ç½‘æœç´¢"
        if config.provider == "deepseek":
            ui_hints += "\nğŸ’¡ **DeepSeek ç”¨æˆ·:** å¯åœ¨è®¾ç½®é¢æ¿ä¸­åˆ‡æ¢å¯¹è¯/æ¨ç†æ¨¡å‹"
        
        config_msg = f"""# âš™ï¸ Current Configuration

**Provider:** {config.provider}
{model_info}
**Temperature:** {config.temperature}
**Max Tokens:** {config.max_tokens}
**Top P:** {config.top_p}
**Timeout:** {config.timeout}s

**Conversation Mode:** {mode_emoji} {mode_name}
**Web Search:** {search_status}

**Available Providers:** {', '.join(cl.user_session.get('available_providers', []))}

{ui_hints}
"""
        await cl.Message(content=config_msg, author="System").send()
    
    elif cmd == "/reset":
        cl.user_session.set("conversation_history", [])
        await cl.Message(
            content="âœ… Conversation history cleared.",
            author="System",
        ).send()
    
    elif cmd == "/mode":
        if not args:
            current_mode = cl.user_session.get("conversation_mode", "chat")
            mode_display = "ğŸ¤– Agent æ¨¡å¼" if current_mode == "agent" else "ğŸ’¬ Chat æ¨¡å¼"
            await cl.Message(
                content=f"""# ğŸ”€ Current Conversation Mode

**Current Mode:** {mode_display}

**Available Modes:**
- `chat` - ğŸ’¬ å¸¸è§„å¯¹è¯ï¼Œå¯æ‰‹åŠ¨æ§åˆ¶æœç´¢
- `agent` - ğŸ¤– è‡ªä¸»å†³ç­–æœç´¢å·¥å…·ä½¿ç”¨

To change: `/mode chat` or `/mode agent`
""",
                author="System",
            ).send()
            return
        
        mode = args.lower().strip()
        if mode not in ["chat", "agent"]:
            await cl.Message(
                content="âš ï¸ Invalid mode. Use `/mode chat` or `/mode agent`",
                author="System",
            ).send()
            return
        
        current_mode = cl.user_session.get("conversation_mode", "chat")
        if mode == current_mode:
            await cl.Message(
                content=f"â„¹ï¸ Already in {mode} mode.",
                author="System",
            ).send()
            return
        
        # Switch mode
        cl.user_session.set("conversation_mode", mode)
        cl.user_session.set("conversation_history", [])
        
        # Initialize agent if switching to agent mode
        if mode == "agent":
            model_wrapper = cl.user_session.get("model_wrapper")
            search_service = cl.user_session.get("search_service")
            
            if model_wrapper and search_service:
                try:
                    search_tool = create_search_tool(search_service)
                    agent_config = get_agent_config()
                    
                    # Create LLMs from config (supports dual LLM mode)
                    current_provider = cl.user_session.get("current_provider", "openai")
                    function_call_llm, answer_llm = create_agent_llms_from_config(
                        default_provider=current_provider,
                        agent_config=agent_config
                    )
                    
                    agent = ReActAgent(
                        llm=function_call_llm,
                        search_tool=search_tool,
                        config=agent_config,
                        answer_llm=answer_llm,
                    )
                    cl.user_session.set("agent", agent)
                    
                    await cl.Message(
                        content="âœ… å·²åˆ‡æ¢åˆ° ğŸ¤– **Agent æ¨¡å¼**\n\næ¨¡å‹ä¼šè‡ªä¸»å†³ç­–ä½•æ—¶ä½¿ç”¨æœç´¢å·¥å…·ã€‚\nå¯¹è¯å†å²å·²æ¸…é™¤ã€‚",
                        author="System",
                    ).send()
                except Exception as e:
                    logger.error(f"Failed to initialize agent: {e}")
                    await cl.Message(
                        content=f"âŒ Agent åˆå§‹åŒ–å¤±è´¥: {str(e)}\n\nå·²å›é€€åˆ° Chat æ¨¡å¼ã€‚",
                        author="System",
                    ).send()
                    cl.user_session.set("conversation_mode", "chat")
            else:
                await cl.Message(
                    content="âš ï¸ æœç´¢æœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•å¯ç”¨ Agent æ¨¡å¼ã€‚\n\nè¯·å…ˆé…ç½® SearXNGã€‚",
                    author="System",
                ).send()
                cl.user_session.set("conversation_mode", "chat")
        else:
            await cl.Message(
                content="âœ… å·²åˆ‡æ¢åˆ° ğŸ’¬ **Chat æ¨¡å¼**\n\nå¯æ‰‹åŠ¨æ§åˆ¶è”ç½‘æœç´¢ã€‚\nå¯¹è¯å†å²å·²æ¸…é™¤ã€‚",
                author="System",
            ).send()
    
    elif cmd == "/search":
        if not args:
            search_enabled = cl.user_session.get("search_enabled", False)
            search_service = cl.user_session.get("search_service")
            
            if not search_service:
                await cl.Message(
                    content="âš ï¸ Web search is not available. Please check your SEARXNG_URL configuration.",
                    author="System",
                ).send()
                return
            
            status = "âœ… Enabled" if search_enabled else "âŒ Disabled"
            await cl.Message(
                content=f"""# ğŸ” Web Search Status

**Current Status:** {status}

To change: `/search on` or `/search off`
""",
                author="System",
            ).send()
            return
        
        action = args.lower().strip()
        search_service = cl.user_session.get("search_service")
        
        if not search_service:
            await cl.Message(
                content="âš ï¸ Web search is not available. Please check your SEARXNG_URL configuration.",
                author="System",
            ).send()
            return
        
        if action == "on":
            cl.user_session.set("search_enabled", True)
            await cl.Message(
                content="âœ… Web search **enabled**\n\nNow you can ask questions that require real-time information!",
                author="System",
            ).send()
        elif action == "off":
            cl.user_session.set("search_enabled", False)
            await cl.Message(
                content="âŒ Web search **disabled**\n\nThe AI will respond using its built-in knowledge only.",
                author="System",
            ).send()
        else:
            await cl.Message(
                content="âš ï¸ Invalid option. Use `/search on` or `/search off`",
                author="System",
            ).send()
    
    elif cmd == "/switch":
        if not args:
            await cl.Message(
                content="âš ï¸ Please specify a provider: `/switch <provider>`\n\nAvailable: openai, anthropic, deepseek",
                author="System",
            ).send()
            return
        
        provider = args.lower().strip()
        available = cl.user_session.get("available_providers", [])
        
        if provider not in available:
            await cl.Message(
                content=f"âš ï¸ Provider '{provider}' not available or not configured.\n\nAvailable providers: {', '.join(available)}",
                author="System",
            ).send()
            return
        
        try:
            # Get cached model wrapper (or create if first time for this provider)
            model_wrapper = await get_or_create_model_wrapper(provider=provider)
            config = model_wrapper.config
            
            # Update session
            cl.user_session.set("model_wrapper", model_wrapper)
            cl.user_session.set("current_provider", provider)
            
            # Clear history
            cl.user_session.set("conversation_history", [])
            
            await cl.Message(
                content=f"""âœ… Switched to **{provider}**

**Model:** {config.model_name}
**Temperature:** {config.temperature}
**Max Tokens:** {config.max_tokens}

Conversation history has been cleared.
""",
                author="System",
            ).send()
        
        except Exception as e:
            logger.error(f"Error switching provider: {str(e)}")
            await cl.Message(
                content=f"âŒ Error switching provider: {str(e)}",
                author="System",
            ).send()
    
    else:
        await cl.Message(
            content=f"âš ï¸ Unknown command: {cmd}\n\nType `/help` for available commands.",
            author="System",
        ).send()


if __name__ == "__main__":
    # This is handled by Chainlit CLI
    pass

