# Agent æ¨¡å¼ï¼šå• LLM vs åŒ LLM è¯¦è§£

## ğŸ“š æ¦‚å¿µè§£é‡Š

### ğŸ¤– å• LLM æ¨¡å¼ï¼ˆSingle LLM Modeï¼‰

**å®šä¹‰**ï¼šä½¿ç”¨åŒä¸€ä¸ª LLM å®Œæˆæ‰€æœ‰ä»»åŠ¡
- æ—¢è´Ÿè´£**å·¥å…·è°ƒç”¨å†³ç­–**ï¼ˆæ€è€ƒã€é€‰æ‹©å·¥å…·ï¼‰
- åˆè´Ÿè´£**æœ€ç»ˆç­”æ¡ˆç”Ÿæˆ**ï¼ˆåŸºäºå·¥å…·ç»“æœå›ç­”ç”¨æˆ·ï¼‰

**å·¥ä½œæµç¨‹**ï¼š
```
ç”¨æˆ·é—®é¢˜ â†’ LLM â†’ æ€è€ƒ â†’ è°ƒç”¨å·¥å…· â†’ è§‚å¯Ÿç»“æœ â†’ LLM â†’ ç”Ÿæˆç­”æ¡ˆ
          â†‘_____________åŒä¸€ä¸ª LLM_______________â†‘
```

### ğŸ”„ åŒ LLM æ¨¡å¼ï¼ˆDual LLM Modeï¼‰

**å®šä¹‰**ï¼šä½¿ç”¨ä¸¤ä¸ªä¸åŒçš„ LLM åˆ†åˆ«å®Œæˆä¸åŒä»»åŠ¡
- **Function Call LLM**ï¼šè´Ÿè´£å·¥å…·è°ƒç”¨å†³ç­–ï¼ˆæ€è€ƒã€é€‰æ‹©å·¥å…·ï¼‰
- **Answer LLM**ï¼šè´Ÿè´£æœ€ç»ˆç­”æ¡ˆç”Ÿæˆï¼ˆåŸºäºå·¥å…·ç»“æœå›ç­”ç”¨æˆ·ï¼‰

**å·¥ä½œæµç¨‹**ï¼š
```
ç”¨æˆ·é—®é¢˜ â†’ Function Call LLM â†’ æ€è€ƒ â†’ è°ƒç”¨å·¥å…· â†’ è§‚å¯Ÿç»“æœ
                                                          â†“
                                    Answer LLM â† ç”Ÿæˆç­”æ¡ˆ
```

---

## ğŸ”§ è§¦å‘æ¡ä»¶

### âœ… è§¦å‘åŒ LLM æ¨¡å¼çš„æ¡ä»¶

**å¿…è¦æ¡ä»¶**ï¼šåœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½® `AGENT_ANSWER_MODEL`

```bash
# .env æ–‡ä»¶ç¤ºä¾‹
# å·¥å…·è°ƒç”¨æ¨¡å‹ï¼ˆè´Ÿè´£æ€è€ƒå’Œé€‰æ‹©å·¥å…·ï¼‰
AGENT_FUNCTION_CALL_MODEL='{"provider": "deepseek", "model_name": "deepseek-chat", "temperature": 0.3}'

# ç­”æ¡ˆç”Ÿæˆæ¨¡å‹ï¼ˆè´Ÿè´£ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼‰- å…³é”®ï¼
AGENT_ANSWER_MODEL='{"provider": "openai", "model_name": "gpt-4o", "temperature": 0.7}'
```

**åˆ¤æ–­é€»è¾‘**ï¼ˆä»£ç ä½ç½®ï¼š`src/agents/react_agent.py:257-258`ï¼‰ï¼š
```python
self.function_call_llm = llm
self.answer_llm = answer_llm if answer_llm is not None else llm

# è¿è¡Œæ—¶åˆ¤æ–­
using_dual_llm = self.answer_llm is not self.function_call_llm
```

**æ£€æµ‹ç‚¹**ï¼ˆä»£ç ä½ç½®ï¼š`src/agents/react_agent.py:453, 617`ï¼‰ï¼š
```python
# åœ¨ run() å’Œ stream() æ–¹æ³•ä¸­
using_dual_llm = self.answer_llm is not self.function_call_llm

if using_dual_llm:
    # ä½¿ç”¨åŒ LLM æ¨¡å¼æµç¨‹
    logger.info("ğŸ”„ åˆ‡æ¢åˆ° answer_llm ç”Ÿæˆæœ€ç»ˆå›ç­”...")
else:
    # ä½¿ç”¨å• LLM æ¨¡å¼æµç¨‹
    logger.info("âœ… ä½¿ç”¨ function_call_llm ç”Ÿæˆæœ€ç»ˆå›ç­”")
```

### âŒ è§¦å‘å• LLM æ¨¡å¼çš„æ¡ä»¶

**æƒ…å†µ 1ï¼šæœªè®¾ç½® `AGENT_ANSWER_MODEL`**
```bash
# .env æ–‡ä»¶
# åªè®¾ç½®ä¸€ä¸ªæ¨¡å‹
AGENT_FUNCTION_CALL_MODEL='{"provider": "deepseek", "model_name": "deepseek-chat"}'
# AGENT_ANSWER_MODEL æœªè®¾ç½®æˆ–ä¸ºç©º
```

**æƒ…å†µ 2ï¼šä¸¤ä¸ªç¯å¢ƒå˜é‡éƒ½æœªè®¾ç½®**
```bash
# .env æ–‡ä»¶
# ä¸¤ä¸ªéƒ½ä¸è®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
# AGENT_FUNCTION_CALL_MODEL æœªè®¾ç½®
# AGENT_ANSWER_MODEL æœªè®¾ç½®
```
åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒAgent ä¼šä½¿ç”¨å½“å‰ç•Œé¢é€‰æ‹©çš„é»˜è®¤æ¨¡å‹ï¼ˆå¦‚ OpenAI GPT-4oï¼‰ã€‚

---

## ğŸ¯ ä½¿ç”¨åœºæ™¯å¯¹æ¯”

### ğŸš€ æ¨èä½¿ç”¨åŒ LLM æ¨¡å¼çš„åœºæ™¯

1. **ä¸“ä¸šåˆ†å·¥ï¼Œå„å¸å…¶èŒ**
   ```
   Function Call LLM: DeepSeek-Chatï¼ˆæ“…é•¿æ¨ç†å’Œå·¥å…·é€‰æ‹©ï¼‰
   Answer LLM: GPT-4oï¼ˆæ“…é•¿ç”Ÿæˆæµç•…çš„å›ç­”ï¼‰
   ```

2. **æˆæœ¬ä¼˜åŒ–**
   ```
   Function Call LLM: ä¾¿å®œçš„æ¨¡å‹ï¼ˆå¦‚ DeepSeekï¼‰- å¤„ç†å·¥å…·è°ƒç”¨
   Answer LLM: æ˜‚è´µä½†é«˜è´¨é‡çš„æ¨¡å‹ï¼ˆå¦‚ GPT-4oï¼‰- åªåœ¨æœ€åç”Ÿæˆç­”æ¡ˆ
   ```

3. **æ€§èƒ½ä¼˜åŒ–**
   ```
   Function Call LLM: å¿«é€Ÿå“åº”çš„æ¨¡å‹ - å¿«é€Ÿå†³ç­–
   Answer LLM: æ…¢ä½†è´¨é‡é«˜çš„æ¨¡å‹ - ç”Ÿæˆé«˜è´¨é‡ç­”æ¡ˆ
   ```

4. **ç‰¹å®šèƒ½åŠ›ç»„åˆ**
   ```
   Function Call LLM: DeepSeek Reasonerï¼ˆæ·±åº¦æ€è€ƒï¼‰- å¤æ‚æ¨ç†
   Answer LLM: GPT-4oï¼ˆæµç•…è¡¨è¾¾ï¼‰- ç”¨æˆ·å‹å¥½çš„å›ç­”
   ```

### âš¡ ä½¿ç”¨å• LLM æ¨¡å¼çš„åœºæ™¯

1. **ç®€åŒ–é…ç½®**
   - ä¸éœ€è¦é…ç½®ä¸¤ä¸ªæ¨¡å‹
   - é€‚åˆå¿«é€Ÿå¼€å§‹å’Œæµ‹è¯•

2. **æ¨¡å‹æœ¬èº«å¾ˆå¼ºå¤§**
   - ä½¿ç”¨ GPT-4o æˆ– Claude 3.5 Sonnet
   - æ—¢èƒ½åšå¥½æ¨ç†ï¼Œåˆèƒ½ç”Ÿæˆå¥½ç­”æ¡ˆ

3. **é™ä½å»¶è¿Ÿ**
   - åªéœ€è¦åŠ è½½ä¸€ä¸ªæ¨¡å‹
   - å‡å°‘æ¨¡å‹åˆ‡æ¢å¼€é”€

---

## ğŸ” ä»£ç å®ç°ç»†èŠ‚

### é…ç½®åŠ è½½ï¼ˆ`src/config/agent_config.py:155-212`ï¼‰

```python
def create_agent_llms_from_config(
    default_provider: str,
    agent_config: Optional[AgentConfig] = None
) -> Tuple[BaseChatModel, Optional[BaseChatModel]]:
    """Create LLM instances for Agent from configuration.
    
    Returns:
        Tuple of (function_call_llm, answer_llm)
        If answer_llm config is not provided, returns None for answer_llm
    """
    # 1. åˆ›å»º function_call_llmï¼ˆå¿…é¡»ï¼‰
    function_call_llm = ...
    
    # 2. åˆ›å»º answer_llmï¼ˆå¯é€‰ï¼‰
    answer_llm = None
    answer_config_json = agent_config.answer_model_config
    if answer_config_json:  # å¦‚æœè®¾ç½®äº† AGENT_ANSWER_MODEL
        answer_llm = ...  # åˆ›å»ºç‹¬ç«‹çš„ answer_llm
    
    return function_call_llm, answer_llm
```

### Agent åˆå§‹åŒ–ï¼ˆ`src/agents/react_agent.py:239-258`ï¼‰

```python
def __init__(
    self,
    llm: BaseChatModel,  # function_call_llm
    search_tool: SearchTool,
    config: Optional[AgentConfig] = None,
    answer_llm: Optional[BaseChatModel] = None,  # å¯é€‰ï¼
    additional_tools: Optional[List[BaseTool]] = None,
):
    self.function_call_llm = llm
    
    # å…³é”®é€»è¾‘ï¼šå¦‚æœæ²¡æœ‰æä¾› answer_llmï¼Œä½¿ç”¨ function_call_llm
    self.answer_llm = answer_llm if answer_llm is not None else llm
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºåŒ LLM æ¨¡å¼
    using_dual_llm = answer_llm is not None
```

### æ‰§è¡Œæµç¨‹å·®å¼‚

#### åŒ LLM æ¨¡å¼æµç¨‹ï¼ˆ`src/agents/react_agent.py:534-539, 805-891`ï¼‰

```python
if using_dual_llm:
    # Step 1: Function Call LLM æ€è€ƒå¹¶è°ƒç”¨å·¥å…·
    # (ç”± LangGraph agent_executor è‡ªåŠ¨å¤„ç†)
    tool_results = [...]  # æ”¶é›†æ‰€æœ‰å·¥å…·æ‰§è¡Œç»“æœ
    
    # Step 2: åˆ‡æ¢åˆ° Answer LLM ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    logger.info("ğŸ”„ åˆ‡æ¢åˆ° answer_llm ç”Ÿæˆæœ€ç»ˆå›ç­”...")
    
    # æ„é€ æç¤ºè¯ï¼ŒåŒ…å«æ‰€æœ‰å·¥å…·ç»“æœ
    system_prompt = """åŸºäºä»¥ä¸‹æœç´¢ç»“æœï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ç­”æ¡ˆ..."""
    user_prompt = f"""ç”¨æˆ·é—®é¢˜: {user_input}
    
æœç´¢ç»“æœ:
{tool_results}

è¯·å›ç­”..."""
    
    # ä½¿ç”¨ answer_llm ç”Ÿæˆç­”æ¡ˆ
    final_answer = await self.answer_llm.ainvoke(messages)
    
    # æ·»åŠ å¼•ç”¨åˆ—è¡¨
    citations_list = self.citation_manager.generate_citations_list(...)
    return final_answer + citations_list
```

#### å• LLM æ¨¡å¼æµç¨‹ï¼ˆ`src/agents/react_agent.py:543-556, 892-957`ï¼‰

```python
elif not using_dual_llm:
    # Function Call LLM æ—¢æ€è€ƒã€è°ƒç”¨å·¥å…·ï¼Œåˆç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    # (ç”± LangGraph agent_executor è‡ªåŠ¨å¤„ç†)
    
    # ä» agent_executor çš„æ¶ˆæ¯ä¸­æå–æœ€ç»ˆç­”æ¡ˆ
    final_answer = ...  # ä» AIMessage.content ä¸­æå–
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç­”æ¡ˆï¼Œå›é€€åˆ°åŒ LLM æ¨¡å¼çš„æ–¹æ³•
    if not final_answer:
        logger.warning("âš ï¸ Agent æœªç”Ÿæˆæœ€ç»ˆå›ç­”ï¼Œä½¿ç”¨ answer_llm ç”Ÿæˆ...")
        final_answer = await self._generate_answer_with_answer_llm(...)
    
    # æ·»åŠ å¼•ç”¨å¤„ç†
    if self.citation_manager and tool_results:
        # è½¬æ¢å¼•ç”¨ [num] â†’ [[num]](url)
        # ç”Ÿæˆå¼•ç”¨åˆ—è¡¨
        ...
    
    return final_answer
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| ç»´åº¦ | å• LLM æ¨¡å¼ | åŒ LLM æ¨¡å¼ |
|------|------------|-----------|
| **é…ç½®å¤æ‚åº¦** | â­ ç®€å• | â­â­ ä¸­ç­‰ |
| **å¯åŠ¨é€Ÿåº¦** | âš¡âš¡âš¡ å¿« | âš¡âš¡ ä¸­ç­‰ï¼ˆéœ€åŠ è½½ä¸¤ä¸ªæ¨¡å‹ï¼‰ |
| **æ‰§è¡Œå»¶è¿Ÿ** | âš¡âš¡âš¡ ä½ | âš¡âš¡ ç¨é«˜ï¼ˆæ¨¡å‹åˆ‡æ¢ï¼‰ |
| **ç­”æ¡ˆè´¨é‡** | â­â­ å–å†³äºå•ä¸ªæ¨¡å‹ | â­â­â­ å¯é€‰æ‹©æœ€ä½³ç»„åˆ |
| **æˆæœ¬æ§åˆ¶** | â­â­ ä¸­ç­‰ | â­â­â­ çµæ´»ï¼ˆä¾¿å®œ+è´µï¼‰ |
| **çµæ´»æ€§** | â­â­ æœ‰é™ | â­â­â­ é«˜ |

---

## ğŸ’¡ å®é™…é…ç½®ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šé«˜æ€§ä»·æ¯”é…ç½®ï¼ˆæ¨èï¼‰

```bash
# ä½¿ç”¨ DeepSeek å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆä¾¿å®œã€å¿«é€Ÿã€æ¨ç†èƒ½åŠ›å¼ºï¼‰
AGENT_FUNCTION_CALL_MODEL='{"provider": "deepseek", "model_name": "deepseek-chat", "temperature": 0.3, "max_tokens": 1000}'

# ä½¿ç”¨ GPT-4o ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼ˆè´¨é‡é«˜ã€è¡¨è¾¾æµç•…ï¼‰
AGENT_ANSWER_MODEL='{"provider": "openai", "model_name": "gpt-4o", "temperature": 0.7, "max_tokens": 2000}'
```

**ä¼˜åŠ¿**ï¼š
- âœ… å·¥å…·è°ƒç”¨é˜¶æ®µä¾¿å®œï¼ˆDeepSeekï¼‰
- âœ… æœ€ç»ˆç­”æ¡ˆè´¨é‡é«˜ï¼ˆGPT-4oï¼‰
- âœ… æ€»ä½“æˆæœ¬é™ä½ 50-70%

### ç¤ºä¾‹ 2ï¼šå…¨èƒ½å•æ¨¡å‹é…ç½®

```bash
# åªä½¿ç”¨ GPT-4o
AGENT_FUNCTION_CALL_MODEL='{"provider": "openai", "model_name": "gpt-4o", "temperature": 0.5}'
# AGENT_ANSWER_MODEL ä¸è®¾ç½®
```

**ä¼˜åŠ¿**ï¼š
- âœ… é…ç½®ç®€å•
- âœ… è´¨é‡ç¨³å®š
- âœ… å»¶è¿Ÿæœ€ä½

### ç¤ºä¾‹ 3ï¼šæè‡´æ¨ç†é…ç½®

```bash
# ä½¿ç”¨ DeepSeek Reasoner æ·±åº¦æ€è€ƒ
AGENT_FUNCTION_CALL_MODEL='{"provider": "deepseek", "model_name": "deepseek-reasoner", "temperature": 0.2}'

# ä½¿ç”¨ Claude ç”Ÿæˆä¼˜é›…ç­”æ¡ˆ
AGENT_ANSWER_MODEL='{"provider": "anthropic", "model_name": "claude-3-5-sonnet-20241022", "temperature": 0.7}'
```

**ä¼˜åŠ¿**ï¼š
- âœ… æœ€å¼ºæ¨ç†èƒ½åŠ›ï¼ˆDeepSeek Reasonerï¼‰
- âœ… æœ€ä½³è¡¨è¾¾èƒ½åŠ›ï¼ˆClaudeï¼‰
- âš ï¸ æˆæœ¬å’Œå»¶è¿Ÿè¾ƒé«˜

### ç¤ºä¾‹ 4ï¼šé»˜è®¤é…ç½®ï¼ˆæ— ç¯å¢ƒå˜é‡ï¼‰

```bash
# .env ä¸­ä¸è®¾ç½®ä»»ä½• Agent æ¨¡å‹é…ç½®
# AGENT_FUNCTION_CALL_MODEL ä¸è®¾ç½®
# AGENT_ANSWER_MODEL ä¸è®¾ç½®
```

**è¡Œä¸º**ï¼š
- ä½¿ç”¨ç•Œé¢é€‰æ‹©çš„é»˜è®¤æ¨¡å‹ï¼ˆå¦‚ OpenAI GPT-4oï¼‰
- å• LLM æ¨¡å¼è¿è¡Œ

---

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜ 1ï¼šå¦‚ä½•ç¡®è®¤å½“å‰ä½¿ç”¨çš„æ˜¯å“ªç§æ¨¡å¼ï¼Ÿ

**æŸ¥çœ‹æ—¥å¿—**ï¼š
```
# å• LLM æ¨¡å¼
ğŸš€ Agent initialized (max_iterations=10, max_execution_time=120s, dual_llm_mode=False, tools=...)

# åŒ LLM æ¨¡å¼
ğŸš€ Agent initialized (max_iterations=10, max_execution_time=120s, dual_llm_mode=True, tools=...)
```

**æˆ–åœ¨æ‰§è¡Œæ—¶çœ‹åˆ°**ï¼š
```
# åŒ LLM æ¨¡å¼
ğŸ”„ åˆ‡æ¢åˆ° answer_llm ç”Ÿæˆæœ€ç»ˆå›ç­”...

# å• LLM æ¨¡å¼
âœ… ä½¿ç”¨ function_call_llm ç”Ÿæˆæœ€ç»ˆå›ç­”
```

### é—®é¢˜ 2ï¼šè®¾ç½®äº† `AGENT_ANSWER_MODEL` ä½†æ²¡ç”Ÿæ•ˆï¼Ÿ

**æ£€æŸ¥æ¸…å•**ï¼š
1. âœ… ç¯å¢ƒå˜é‡æ ¼å¼æ­£ç¡®ï¼ˆJSON å­—ç¬¦ä¸²ï¼‰
2. âœ… é‡å¯äº†åº”ç”¨ï¼ˆä¿®æ”¹ `.env` éœ€è¦é‡å¯ï¼‰
3. âœ… æ£€æŸ¥æ—¥å¿—ä¸­çš„ `dual_llm_mode` å€¼

**éªŒè¯æ–¹æ³•**ï¼š
```bash
# æ‰“å°ç¯å¢ƒå˜é‡
echo $AGENT_ANSWER_MODEL

# æˆ–åœ¨ Python ä¸­
import os
print(os.getenv("AGENT_ANSWER_MODEL"))
```

### é—®é¢˜ 3ï¼šåŒ LLM æ¨¡å¼æŠ¥é”™ï¼Ÿ

**å¸¸è§é”™è¯¯**ï¼š
```
âŒ Failed to initialize answer_llm: Invalid provider
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ£€æŸ¥ `provider` æ˜¯å¦æ­£ç¡®ï¼ˆ`openai`, `anthropic`, `deepseek`ï¼‰
- æ£€æŸ¥å¯¹åº”çš„ API Key æ˜¯å¦é…ç½®ï¼ˆ`OPENAI_API_KEY` ç­‰ï¼‰
- æ£€æŸ¥ JSON æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼ˆä½¿ç”¨å•å¼•å·åŒ…è£¹ï¼Œå†…éƒ¨ä½¿ç”¨åŒå¼•å·ï¼‰

---

## ğŸ“ æ€»ç»“

| ç‰¹æ€§ | å• LLM æ¨¡å¼ | åŒ LLM æ¨¡å¼ |
|------|-----------|-----------|
| **è§¦å‘æ¡ä»¶** | ä¸è®¾ç½® `AGENT_ANSWER_MODEL` | è®¾ç½® `AGENT_ANSWER_MODEL` |
| **é€‚ç”¨åœºæ™¯** | ç®€å•å¿«é€Ÿã€å•ä¸€å¼ºæ¨¡å‹ | ä¸“ä¸šåˆ†å·¥ã€æˆæœ¬ä¼˜åŒ– |
| **æ¨èäººç¾¤** | æ–°æ‰‹ã€å¿«é€Ÿæµ‹è¯• | é«˜çº§ç”¨æˆ·ã€ç”Ÿäº§ç¯å¢ƒ |
| **æœ€ä½³å®è·µ** | GPT-4o å•æ¨¡å‹ | DeepSeek + GPT-4o ç»„åˆ |

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [Agent æ¨¡å¼è§„èŒƒ](../../openspec/specs/agent-mode/spec.md)
- [åŒ LLM å®ç°æ€»ç»“](../../openspec/changes/archive/2025-12-29-separate-function-call-and-answer-llms/IMPLEMENTATION_SUMMARY.md)
- [Agent é…ç½®æŒ‡å—](../../openspec/specs/agent-mode/spec.md#configuration)

---

## ğŸ”— ä»£ç å‚è€ƒ

- **é…ç½®åŠ è½½**ï¼š`src/config/agent_config.py`
- **Agent å®ç°**ï¼š`src/agents/react_agent.py`
- **åº”ç”¨é›†æˆ**ï¼š`app.py`

