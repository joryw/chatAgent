"""DeepSeek model wrapper implementation."""

import logging
import re
from typing import AsyncIterator, Optional, Any, Dict, List

import tiktoken
from langchain_openai import ChatOpenAI
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import BaseMessage, AIMessage, ToolMessage
from langchain_core.outputs import ChatGeneration, ChatResult
from openai import OpenAI
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)

from ..config.model_config import ModelConfig
from .base import BaseModelWrapper, ModelResponse, StreamChunk

logger = logging.getLogger(__name__)





def _add_reasoning_content_to_messages_helper(messages):
    """Helper function to add reasoning_content to messages.
    
    This function ensures all assistant messages with tool_calls have reasoning_content,
    which is required by DeepSeek API. It handles both dict and BaseMessage formats.
    
    This is a standalone function that can be used by all code paths to ensure
    consistent message processing.
    
    IMPORTANT: For BaseMessage objects, we modify them in-place to preserve all fields
    (like tool_call_id for ToolMessage). Only dict messages are converted.
    
    Args:
        messages: List of message dicts or BaseMessage objects
        
    Returns:
        Modified list of messages (dict messages remain as dict, BaseMessage objects remain as BaseMessage)
    """
    modified = []
    for i, msg in enumerate(messages):
        # Handle dict format (most common in API calls)
        if isinstance(msg, dict):
            msg_copy = msg.copy()
            role = msg_copy.get("role")
            tool_calls = msg_copy.get("tool_calls")
            
            # Log message details for debugging
            logger.debug(f"å¤„ç†æ¶ˆæ¯ç´¢å¼• {i}: role={role}, has_tool_calls={bool(tool_calls)}, has_reasoning={bool(msg_copy.get('reasoning_content'))}")
            
            # CRITICAL: Add reasoning_content for ALL assistant messages with tool_calls
            # DeepSeek API requires this field when tool_calls are present
            if role == "assistant" and tool_calls:
                if "reasoning_content" not in msg_copy:
                    reasoning = msg_copy.get("content", "")
                    if not reasoning or reasoning.strip() == "":
                        reasoning = "æ­£åœ¨æ€è€ƒå¦‚ä½•ä½¿ç”¨å·¥å…·æ¥å›ç­”è¿™ä¸ªé—®é¢˜..."
                    msg_copy["reasoning_content"] = reasoning
                    logger.info(f"âœ… [æ¶ˆæ¯ç´¢å¼• {i}] æ·»åŠ  reasoning_content (å·¥å…·è°ƒç”¨: {len(tool_calls)} ä¸ª)")
                else:
                    logger.debug(f"æ¶ˆæ¯ç´¢å¼• {i} å·²æœ‰ reasoning_content")
            
            # Also check for assistant messages in tool-calling context
            # Sometimes DeepSeek requires reasoning_content even without explicit tool_calls
            # if it's part of a tool-calling conversation
            elif role == "assistant" and i > 0:
                # Check if previous messages indicate tool-calling context
                prev_msg = messages[i-1] if i > 0 else None
                if isinstance(prev_msg, dict) and prev_msg.get("role") == "assistant" and prev_msg.get("tool_calls"):
                    # This might be a follow-up assistant message in tool-calling flow
                    if "reasoning_content" not in msg_copy:
                        reasoning = msg_copy.get("content", "")
                        if not reasoning or reasoning.strip() == "":
                            reasoning = "æ­£åœ¨å¤„ç†å·¥å…·è°ƒç”¨ç»“æœ..."
                        msg_copy["reasoning_content"] = reasoning
                        logger.info(f"âœ… [æ¶ˆæ¯ç´¢å¼• {i}] æ·»åŠ ä¸Šä¸‹æ–‡ reasoning_content (å·¥å…·è°ƒç”¨æµç¨‹)")
            
            modified.append(msg_copy)
        
        # Handle BaseMessage format (from LangChain)
        elif isinstance(msg, BaseMessage):
            # IMPORTANT: For BaseMessage, we should modify the object directly
            # instead of converting to dict, to preserve all fields (like tool_call_id for ToolMessage)
            
            # Only process AIMessage with tool_calls
            if isinstance(msg, AIMessage):
                # Extract tool_calls
                tool_calls = None
                if hasattr(msg, 'tool_calls') and msg.tool_calls:
                    tool_calls = msg.tool_calls
                elif hasattr(msg, 'additional_kwargs') and msg.additional_kwargs:
                    tool_calls = msg.additional_kwargs.get('tool_calls')
                
                # Check for existing reasoning_content
                existing_reasoning = None
                if hasattr(msg, 'additional_kwargs') and msg.additional_kwargs:
                    existing_reasoning = msg.additional_kwargs.get('reasoning_content')
                
                # CRITICAL: Add reasoning_content for ALL assistant messages with tool_calls
                if tool_calls:
                    if not existing_reasoning:
                        # Initialize additional_kwargs if needed
                        if not hasattr(msg, 'additional_kwargs') or msg.additional_kwargs is None:
                            msg.additional_kwargs = {}
                        
                        reasoning = msg.content if hasattr(msg, 'content') and msg.content else ""
                        if not reasoning or reasoning.strip() == "":
                            reasoning = "æ­£åœ¨æ€è€ƒå¦‚ä½•ä½¿ç”¨å·¥å…·æ¥å›ç­”è¿™ä¸ªé—®é¢˜..."
                        
                        msg.additional_kwargs['reasoning_content'] = reasoning
                        # Log tool_calls count for debugging
                        tool_calls_count = len(tool_calls) if isinstance(tool_calls, list) else 1
                        logger.info(f"âœ… [æ¶ˆæ¯ç´¢å¼• {i}] BaseMessage å¯¹è±¡æ·»åŠ  reasoning_content (å·¥å…·è°ƒç”¨: {tool_calls_count} ä¸ª)")
                        logger.debug(f"   reasoning_content å†…å®¹: {reasoning[:100]}...")
                    else:
                        logger.debug(f"æ¶ˆæ¯ç´¢å¼• {i} å·²æœ‰ reasoning_content: {existing_reasoning[:50]}...")
            
            # For all BaseMessage objects (including ToolMessage, HumanMessage, etc.),
            # keep them as-is to preserve all fields
            modified.append(msg)
        else:
            # Unknown format, pass through (but log warning)
            logger.warning(f"âš ï¸ æœªçŸ¥æ¶ˆæ¯æ ¼å¼åœ¨ç´¢å¼• {i}: {type(msg)}")
            modified.append(msg)
    
    return modified


class DeepSeekWrapper(BaseModelWrapper):
    """Wrapper for DeepSeek models using OpenAI-compatible API."""
    
    def __init__(self, config: ModelConfig):
        """Initialize DeepSeek wrapper.
        
        Args:
            config: Model configuration
        """
        super().__init__(config)
        
        # Initialize OpenAI client with DeepSeek base URL
        self.client = OpenAI(
            api_key=config.api_key,
            base_url=config.base_url,
        )
        
        # Initialize LangChain model with DeepSeek base URL
        self.model = ChatOpenAI(
            model=config.model_name,
            temperature=config.temperature,
            max_tokens=config.max_tokens,
            openai_api_key=config.api_key,
            openai_api_base=config.base_url,
            request_timeout=config.timeout,
        )
        
        # Use cl100k_base tokenizer (similar to GPT-4)
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((Exception,)),
        reraise=True,
    )
    async def generate(
        self,
        prompt: str,
        system_message: Optional[str] = None,
        **kwargs
    ) -> ModelResponse:
        """Generate a response using DeepSeek API.
        
        Args:
            prompt: User prompt/message
            system_message: Optional system message for context
            **kwargs: Additional DeepSeek-specific parameters
        
        Returns:
            ModelResponse with generated content and metadata
        
        Raises:
            Exception: If API call fails after retries
        """
        try:
            # Validate context length
            is_valid, token_count = self.validate_context_length(
                prompt, system_message
            )
            if not is_valid:
                logger.warning(
                    f"Prompt length ({token_count} tokens) is close to "
                    f"context limit. Consider reducing prompt size."
                )
            
            # Prepare messages
            messages = []
            if system_message:
                messages.append({"role": "system", "content": system_message})
            messages.append({"role": "user", "content": prompt})
            
            # Override config with kwargs if provided
            temperature = kwargs.get("temperature", self.config.temperature)
            max_tokens = kwargs.get("max_tokens", self.config.max_tokens)
            
            # Call DeepSeek API (OpenAI-compatible)
            response = self.client.chat.completions.create(
                model=self.config.model_name,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                timeout=self.config.timeout,
            )
            
            # Extract response
            content = response.choices[0].message.content
            if not content:
                raise ValueError("Model returned empty response")
            
            # Build structured response
            usage = response.usage
            return ModelResponse(
                content=content,
                model=response.model,
                tokens_used=usage.total_tokens,
                prompt_tokens=usage.prompt_tokens,
                completion_tokens=usage.completion_tokens,
                finish_reason=response.choices[0].finish_reason,
            )
        
        except Exception as e:
            logger.error(f"DeepSeek API call failed: {str(e)}")
            raise
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((Exception,)),
        reraise=True,
    )
    async def generate_stream(
        self,
        prompt: str,
        system_message: Optional[str] = None,
        **kwargs
    ) -> AsyncIterator[StreamChunk]:
        """Generate a streaming response using DeepSeek API.
        
        For deepseek-reasoner model, yields reasoning content first, then answer.
        For deepseek-chat model, yields answer content directly.
        
        Args:
            prompt: User prompt/message
            system_message: Optional system message for context
            **kwargs: Additional DeepSeek-specific parameters
        
        Yields:
            StreamChunk objects containing response text chunks with chunk_type
        
        Raises:
            Exception: If API call fails after retries
        """
        try:
            # Validate context length
            is_valid, token_count = self.validate_context_length(
                prompt, system_message
            )
            if not is_valid:
                logger.warning(
                    f"Prompt length ({token_count} tokens) is close to "
                    f"context limit. Consider reducing prompt size."
                )
            
            # Prepare messages
            messages = []
            if system_message:
                messages.append({"role": "system", "content": system_message})
            messages.append({"role": "user", "content": prompt})
            
            # CRITICAL: Process messages to ensure all assistant messages with tool_calls have reasoning_content
            # This is required by DeepSeek API when tool_calls are present
            messages = _add_reasoning_content_to_messages_helper(messages)
            
            # Override config with kwargs if provided
            temperature = kwargs.get("temperature", self.config.temperature)
            max_tokens = kwargs.get("max_tokens", self.config.max_tokens)
            
            # Call DeepSeek API with streaming
            # Wrap in try-except to handle reasoning_content errors and retry
            try:
                stream = self.client.chat.completions.create(
                    model=self.config.model_name,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    timeout=self.config.timeout,
                    stream=True,
                )
            except Exception as e:
                error_str = str(e)
                # If we get reasoning_content error, try more aggressive fix
                if "reasoning_content" in error_str.lower():
                    logger.warning(f"âš ï¸ [generate_stream] é‡åˆ° reasoning_content é”™è¯¯ï¼Œå°è¯•ä¿®å¤")
                    logger.debug(f"é”™è¯¯è¯¦æƒ…: {error_str[:300]}")
                    
                    # More aggressive fix: ensure ALL assistant messages have reasoning_content
                    for i, msg in enumerate(messages):
                        if isinstance(msg, dict) and msg.get("role") == "assistant":
                            if msg.get("tool_calls") and "reasoning_content" not in msg:
                                reasoning = msg.get("content", "")
                                if not reasoning or reasoning.strip() == "":
                                    reasoning = "æ­£åœ¨æ€è€ƒå¦‚ä½•ä½¿ç”¨å·¥å…·æ¥å›ç­”è¿™ä¸ªé—®é¢˜..."
                                msg["reasoning_content"] = reasoning
                                logger.info(f"âœ… [generate_stream-é”™è¯¯ä¿®å¤-æ¶ˆæ¯ {i}] å¼ºåˆ¶æ·»åŠ  reasoning_content")
                    
                    # Retry with fixed messages
                    stream = self.client.chat.completions.create(
                        model=self.config.model_name,
                        messages=messages,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        timeout=self.config.timeout,
                        stream=True,
                    )
                else:
                    raise
            
            # Check if this is a reasoner model
            is_reasoner = self.config.model_variant == "deepseek-reasoner"
            
            # Stream response chunks
            for chunk in stream:
                delta = chunk.choices[0].delta
                
                # Handle reasoning content (only for deepseek-reasoner)
                if is_reasoner and hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                    yield StreamChunk(
                        content=delta.reasoning_content,
                        finish_reason=None,
                        chunk_type="reasoning",
                    )
                
                # Handle answer content
                if delta.content:
                    yield StreamChunk(
                        content=delta.content,
                        finish_reason=chunk.choices[0].finish_reason,
                        chunk_type="answer",
                    )
        
        except Exception as e:
            logger.error(f"DeepSeek streaming call failed: {str(e)}")
            raise
    
    def count_tokens(self, text: str) -> int:
        """Count tokens using tiktoken.
        
        Args:
            text: Text to count tokens for
        
        Returns:
            Number of tokens
        """
        return len(self.tokenizer.encode(text))
    
    def get_langchain_llm(self):
        """Get LangChain compatible LLM instance with DeepSeek reasoning_content support.
        
        Returns:
            LangChain ChatOpenAI instance configured for DeepSeek
        """
        # Create a custom ChatOpenAI subclass that handles reasoning_content
        class DeepSeekChatOpenAI(ChatOpenAI):
            """Custom ChatOpenAI that adds reasoning_content for DeepSeek API."""
            
            def _add_reasoning_content_to_messages(self, messages):
                """Helper method to add reasoning_content to messages.
                
                This method ensures all assistant messages with tool_calls have reasoning_content,
                which is required by DeepSeek API. It handles both dict and BaseMessage formats.
                
                Args:
                    messages: List of message dicts or BaseMessage objects
                    
                Returns:
                    Modified list of messages
                """
                modified = []
                for i, msg in enumerate(messages):
                    # Handle dict format (most common in API calls)
                    if isinstance(msg, dict):
                        msg_copy = msg.copy()
                        role = msg_copy.get("role")
                        tool_calls = msg_copy.get("tool_calls")
                        
                        # Log message details for debugging
                        logger.debug(f"å¤„ç†æ¶ˆæ¯ç´¢å¼• {i}: role={role}, has_tool_calls={bool(tool_calls)}, has_reasoning={bool(msg_copy.get('reasoning_content'))}")
                        
                        # CRITICAL: Add reasoning_content for ALL assistant messages with tool_calls
                        # DeepSeek API requires this field when tool_calls are present
                        if role == "assistant" and tool_calls:
                            if "reasoning_content" not in msg_copy:
                                reasoning = msg_copy.get("content", "")
                                if not reasoning or reasoning.strip() == "":
                                    reasoning = "æ­£åœ¨æ€è€ƒå¦‚ä½•ä½¿ç”¨å·¥å…·æ¥å›ç­”è¿™ä¸ªé—®é¢˜..."
                                msg_copy["reasoning_content"] = reasoning
                                logger.info(f"âœ… [æ¶ˆæ¯ç´¢å¼• {i}] æ·»åŠ  reasoning_content (å·¥å…·è°ƒç”¨: {len(tool_calls)} ä¸ª)")
                            else:
                                logger.debug(f"æ¶ˆæ¯ç´¢å¼• {i} å·²æœ‰ reasoning_content")
                        
                        # Also check for assistant messages in tool-calling context
                        # Sometimes DeepSeek requires reasoning_content even without explicit tool_calls
                        # if it's part of a tool-calling conversation
                        elif role == "assistant" and i > 0:
                            # Check if previous messages indicate tool-calling context
                            prev_msg = messages[i-1] if i > 0 else None
                            if isinstance(prev_msg, dict) and prev_msg.get("role") == "assistant" and prev_msg.get("tool_calls"):
                                # This might be a follow-up assistant message in tool-calling flow
                                if "reasoning_content" not in msg_copy:
                                    reasoning = msg_copy.get("content", "")
                                    if not reasoning or reasoning.strip() == "":
                                        reasoning = "æ­£åœ¨å¤„ç†å·¥å…·è°ƒç”¨ç»“æœ..."
                                    msg_copy["reasoning_content"] = reasoning
                                    logger.info(f"âœ… [æ¶ˆæ¯ç´¢å¼• {i}] æ·»åŠ ä¸Šä¸‹æ–‡ reasoning_content (å·¥å…·è°ƒç”¨æµç¨‹)")
                        
                        modified.append(msg_copy)
                    
                    # Handle BaseMessage format (from LangChain)
                    elif isinstance(msg, BaseMessage):
                        # IMPORTANT: For BaseMessage, we should modify the object directly
                        # instead of converting to dict, to preserve all fields (like tool_call_id for ToolMessage)
                        
                        # Only process AIMessage with tool_calls
                        if isinstance(msg, AIMessage):
                            # Extract tool_calls
                            tool_calls = None
                            if hasattr(msg, 'tool_calls') and msg.tool_calls:
                                tool_calls = msg.tool_calls
                            elif hasattr(msg, 'additional_kwargs') and msg.additional_kwargs:
                                tool_calls = msg.additional_kwargs.get('tool_calls')
                            
                            # Check for existing reasoning_content
                            existing_reasoning = None
                            if hasattr(msg, 'additional_kwargs') and msg.additional_kwargs:
                                existing_reasoning = msg.additional_kwargs.get('reasoning_content')
                            
                            # CRITICAL: Add reasoning_content for ALL assistant messages with tool_calls
                            if tool_calls:
                                if not existing_reasoning:
                                    # Initialize additional_kwargs if needed
                                    if not hasattr(msg, 'additional_kwargs') or msg.additional_kwargs is None:
                                        msg.additional_kwargs = {}
                                    
                                    reasoning = msg.content if hasattr(msg, 'content') and msg.content else ""
                                    if not reasoning or reasoning.strip() == "":
                                        reasoning = "æ­£åœ¨æ€è€ƒå¦‚ä½•ä½¿ç”¨å·¥å…·æ¥å›ç­”è¿™ä¸ªé—®é¢˜..."
                                    
                                    msg.additional_kwargs['reasoning_content'] = reasoning
                                    # Log tool_calls count for debugging
                                    tool_calls_count = len(tool_calls) if isinstance(tool_calls, list) else 1
                                    logger.info(f"âœ… [æ¶ˆæ¯ç´¢å¼• {i}] BaseMessage å¯¹è±¡æ·»åŠ  reasoning_content (å·¥å…·è°ƒç”¨: {tool_calls_count} ä¸ª)")
                                    logger.debug(f"   reasoning_content å†…å®¹: {reasoning[:100]}...")
                                else:
                                    logger.debug(f"æ¶ˆæ¯ç´¢å¼• {i} å·²æœ‰ reasoning_content: {existing_reasoning[:50]}...")
                        
                        # For all BaseMessage objects (including ToolMessage, HumanMessage, etc.),
                        # keep them as-is to preserve all fields
                        modified.append(msg)
                    else:
                        # Unknown format, pass through
                        modified.append(msg)
                
                return modified
            
            def _wrap_client_create(self, original_create):
                """Create a wrapper for client.create method.
                
                Args:
                    original_create: Original create method
                    
                Returns:
                    Wrapped create method
                """
                def wrapped_create(*args, **create_kwargs):
                    """Wrapper that adds reasoning_content to messages."""
                    if "messages" in create_kwargs:
                        create_kwargs["messages"] = self._add_reasoning_content_to_messages(
                            create_kwargs["messages"]
                        )
                    
                    try:
                        return original_create(*args, **create_kwargs)
                    except Exception as e:
                        error_str = str(e)
                        # If we still get reasoning_content error, try more aggressive fix
                        if "reasoning_content" in error_str.lower() and "messages" in create_kwargs:
                            logger.warning(f"âš ï¸ ä»ç„¶é‡åˆ° reasoning_content é”™è¯¯ï¼Œå°è¯•æ›´æ¿€è¿›çš„ä¿®å¤")
                            logger.debug(f"é”™è¯¯è¯¦æƒ…: {error_str[:200]}")
                            
                            # Try to fix ALL assistant messages, not just those with tool_calls
                            msgs = create_kwargs["messages"]
                            for i, msg in enumerate(msgs):
                                if isinstance(msg, dict) and msg.get("role") == "assistant":
                                    if "reasoning_content" not in msg:
                                        reasoning = msg.get("content", "")
                                        if not reasoning or reasoning.strip() == "":
                                            reasoning = "æ­£åœ¨æ€è€ƒä¸­..."
                                        msg["reasoning_content"] = reasoning
                                        logger.info(f"âœ… [é”™è¯¯ä¿®å¤] æ¶ˆæ¯ç´¢å¼• {i} å¼ºåˆ¶æ·»åŠ  reasoning_content")
                            
                            # Retry with fixed messages
                            return original_create(*args, **create_kwargs)
                        raise
                
                return wrapped_create
            
            def _generate(self, messages, stop=None, run_manager=None, **kwargs):
                """Override _generate to add reasoning_content before API call."""
                # Process messages before passing to parent
                if isinstance(messages, list):
                    messages = _add_reasoning_content_to_messages_helper(messages)
                
                original_create = self.client.create
                wrapped_create = self._wrap_client_create(original_create)
                
                # Replace create method temporarily
                self.client.create = wrapped_create
                try:
                    return super()._generate(messages, stop=stop, run_manager=run_manager, **kwargs)
                finally:
                    # Restore original
                    self.client.create = original_create
            
            async def _agenerate(self, messages, stop=None, run_manager=None, **kwargs):
                """Override _agenerate to add reasoning_content before async API call."""
                # CRITICAL: Process messages BEFORE passing to parent
                # This ensures reasoning_content is added to BaseMessage objects before formatting
                if isinstance(messages, list):
                    messages = _add_reasoning_content_to_messages_helper(messages)
                
                # CRITICAL: Also ensure _format_messages will be called with proper reasoning_content
                # by wrapping the client's chat.completions.create method
                original_create = self.client.create
                wrapped_create = self._wrap_client_create(original_create)
                
                # Replace create method temporarily
                self.client.create = wrapped_create
                try:
                    # Call parent's _agenerate which will call _format_messages
                    # _format_messages will ensure reasoning_content is in the final dict format
                    return await super()._agenerate(messages, stop=stop, run_manager=run_manager, **kwargs)
                finally:
                    # Restore original
                    self.client.create = original_create
            
            def _format_messages(self, messages):
                """Override _format_messages to add reasoning_content before formatting.
                
                This is called by LangChain to convert messages to API format.
                We intercept here to ensure reasoning_content is added.
                """
                # Convert messages to list if needed
                if not isinstance(messages, list):
                    messages = list(messages)
                
                # Process messages to add reasoning_content
                processed_messages = _add_reasoning_content_to_messages_helper(messages)
                
                # CRITICAL: Build a mapping of tool_calls to reasoning_content BEFORE formatting
                # This ensures we can match messages even if LangChain filters or reorders them
                tool_calls_to_reasoning = {}
                for msg in processed_messages:
                    if isinstance(msg, AIMessage):
                        tool_calls = None
                        if hasattr(msg, 'tool_calls') and msg.tool_calls:
                            tool_calls = msg.tool_calls
                        elif hasattr(msg, 'additional_kwargs') and msg.additional_kwargs:
                            tool_calls = msg.additional_kwargs.get('tool_calls')
                        
                        if tool_calls:
                            # Use tool_calls as key (convert to string for hashing)
                            tool_calls_key = str(tool_calls)
                            reasoning = None
                            if hasattr(msg, 'additional_kwargs') and msg.additional_kwargs:
                                reasoning = msg.additional_kwargs.get('reasoning_content')
                            if not reasoning and hasattr(msg, 'content') and msg.content:
                                reasoning = msg.content
                            if not reasoning or reasoning.strip() == "":
                                reasoning = "æ­£åœ¨æ€è€ƒå¦‚ä½•ä½¿ç”¨å·¥å…·æ¥å›ç­”è¿™ä¸ªé—®é¢˜..."
                            tool_calls_to_reasoning[tool_calls_key] = reasoning
                            logger.debug(f"ğŸ” [_format_messages] å»ºç«‹æ˜ å°„: tool_calls -> reasoning_content (é•¿åº¦: {len(reasoning)})")
                
                # Call parent method with processed messages
                # Parent will convert BaseMessage to dict, but may not extract reasoning_content from additional_kwargs
                formatted = super()._format_messages(processed_messages)
                
                # Ensure reasoning_content from additional_kwargs is extracted to dict format
                # LangChain's _format_messages should handle this, but we ensure it here
                if isinstance(formatted, list):
                    for i, msg_dict in enumerate(formatted):
                        if isinstance(msg_dict, dict) and msg_dict.get("role") == "assistant":
                            # If we have tool_calls but no reasoning_content, check if it's in the original message
                            if msg_dict.get("tool_calls") and "reasoning_content" not in msg_dict:
                                # CRITICAL: We MUST add reasoning_content for tool_calls
                                # Try to find reasoning_content in the original message
                                reasoning = None
                                
                                # Match by index first (most reliable)
                                logger.debug(f"ğŸ” [_format_messages] å°è¯•åŒ¹é…æ¶ˆæ¯ç´¢å¼• {i}ï¼Œprocessed_messages é•¿åº¦: {len(processed_messages)}")
                                if i < len(processed_messages):
                                    orig_msg = processed_messages[i]
                                    logger.debug(f"  åŸå§‹æ¶ˆæ¯ç±»å‹: {type(orig_msg)}")
                                    if isinstance(orig_msg, AIMessage):
                                        # Check additional_kwargs first
                                        if hasattr(orig_msg, 'additional_kwargs') and orig_msg.additional_kwargs:
                                            reasoning = orig_msg.additional_kwargs.get('reasoning_content')
                                            logger.debug(f"  ä» additional_kwargs æå– reasoning: {bool(reasoning)}")
                                        # If not found, use content as reasoning
                                        if not reasoning and hasattr(orig_msg, 'content') and orig_msg.content:
                                            reasoning = orig_msg.content
                                            logger.debug(f"  ä½¿ç”¨ content ä½œä¸º reasoning: {bool(reasoning)}")
                                else:
                                    logger.warning(f"âš ï¸ æ¶ˆæ¯ç´¢å¼• {i} è¶…å‡º processed_messages èŒƒå›´")
                                
                                # If still not found, try to match by tool_calls using our mapping
                                if not reasoning:
                                    tool_calls_key = str(tool_calls)
                                    if tool_calls_key in tool_calls_to_reasoning:
                                        reasoning = tool_calls_to_reasoning[tool_calls_key]
                                        logger.debug(f"âœ… [_format_messages] é€šè¿‡ tool_calls æ˜ å°„æ‰¾åˆ° reasoning_content")
                                
                                # If still not found, search all messages by tool_calls match
                                if not reasoning:
                                    for orig_msg in processed_messages:
                                        if isinstance(orig_msg, AIMessage):
                                            # Check if this message has tool_calls (match by tool_calls)
                                            orig_tool_calls = None
                                            if hasattr(orig_msg, 'tool_calls') and orig_msg.tool_calls:
                                                orig_tool_calls = orig_msg.tool_calls
                                            elif hasattr(orig_msg, 'additional_kwargs') and orig_msg.additional_kwargs:
                                                orig_tool_calls = orig_msg.additional_kwargs.get('tool_calls')
                                            
                                            # If tool_calls match, extract reasoning_content
                                            if orig_tool_calls and len(orig_tool_calls) == len(tool_calls):
                                                if hasattr(orig_msg, 'additional_kwargs') and orig_msg.additional_kwargs:
                                                    reasoning = orig_msg.additional_kwargs.get('reasoning_content')
                                                if not reasoning and hasattr(orig_msg, 'content') and orig_msg.content:
                                                    reasoning = orig_msg.content
                                                if reasoning:
                                                    logger.debug(f"âœ… [_format_messages] é€šè¿‡éå†æ‰¾åˆ° reasoning_content")
                                                    break
                                
                                # If still no reasoning, use default
                                if not reasoning or reasoning.strip() == "":
                                    reasoning = "æ­£åœ¨æ€è€ƒå¦‚ä½•ä½¿ç”¨å·¥å…·æ¥å›ç­”è¿™ä¸ªé—®é¢˜..."
                                
                                msg_dict["reasoning_content"] = reasoning
                                logger.info(f"âœ… [_format_messages] æ¶ˆæ¯ç´¢å¼• {i} å¼ºåˆ¶æ·»åŠ  reasoning_content (å·¥å…·è°ƒç”¨: {len(tool_calls)} ä¸ª)")
                
                return formatted
            
            async def _astream(self, messages, stop=None, run_manager=None, **kwargs):
                """Override _astream to add reasoning_content before streaming API call."""
                # CRITICAL: Process messages BEFORE passing to parent
                # This ensures reasoning_content is added to BaseMessage objects before formatting
                if isinstance(messages, list):
                    messages = _add_reasoning_content_to_messages_helper(messages)
                
                # Also ensure _format_messages will be called with proper reasoning_content
                # by wrapping the client's chat.completions.create method
                original_create = self.client.create
                wrapped_create = self._wrap_client_create(original_create)
                
                # Replace create method temporarily
                self.client.create = wrapped_create
                try:
                    async for chunk in super()._astream(messages, stop=stop, run_manager=run_manager, **kwargs):
                        yield chunk
                finally:
                    # Restore original
                    self.client.create = original_create
            
            async def astream(self, input, config=None, **kwargs):
                """Override astream to add reasoning_content before streaming API call.
                
                This is called by LangGraph, so we need to intercept here too.
                LangGraph may pass messages in different formats, so we need to handle them.
                """
                # CRITICAL: Process messages BEFORE LangGraph processes them
                # LangGraph may pass messages as dict or list, handle both cases
                if isinstance(input, dict) and "messages" in input:
                    input["messages"] = _add_reasoning_content_to_messages_helper(input["messages"])
                    logger.debug(f"ğŸ” [astream] å¤„ç†äº† {len(input['messages'])} æ¡æ¶ˆæ¯ (dictæ ¼å¼)")
                elif isinstance(input, list):
                    # Input might be a list of messages
                    input = _add_reasoning_content_to_messages_helper(input)
                    logger.debug(f"ğŸ” [astream] å¤„ç†äº† {len(input)} æ¡æ¶ˆæ¯ (listæ ¼å¼)")
                
                # Wrap client methods to ensure reasoning_content is added at API call time
                original_create = self.client.create
                wrapped_create = self._wrap_client_create(original_create)
                
                # Replace create method temporarily
                self.client.create = wrapped_create
                try:
                    async for chunk in super().astream(input, config=config, **kwargs):
                        yield chunk
                finally:
                    # Restore original
                    self.client.create = original_create
        
        # Get LangSmith callbacks if enabled
        callbacks = self._get_callbacks()
        
        # Create new instance with same configuration
        wrapped_model = DeepSeekChatOpenAI(
            model=self.model.model_name,
            temperature=self.model.temperature,
            max_tokens=self.model.max_tokens,
            openai_api_key=self.model.openai_api_key,
            openai_api_base=self.model.openai_api_base,
            request_timeout=self.model.request_timeout,
            callbacks=callbacks if callbacks else None,
        )
        
        # Also wrap the client's create method proactively for additional safety
        # This ensures all API calls (sync, async, streaming) are intercepted
        original_create = wrapped_model.client.create
        
        def proactive_wrapped_create(*args, **kwargs):
            """Proactively add reasoning_content before API call.
            
            This wrapper ensures all assistant messages with tool_calls have reasoning_content
            before the API call is made, preventing errors from DeepSeek API.
            """
            if "messages" in kwargs:
                # Use the standalone helper function for consistent processing
                kwargs["messages"] = _add_reasoning_content_to_messages_helper(kwargs["messages"])
                
                # Verify all assistant messages with tool_calls have reasoning_content
                for i, msg in enumerate(kwargs["messages"]):
                    if isinstance(msg, dict) and msg.get("role") == "assistant" and msg.get("tool_calls"):
                        if "reasoning_content" not in msg:
                            logger.warning(f"âš ï¸ [ä¸»åŠ¨åŒ…è£…-éªŒè¯å¤±è´¥] æ¶ˆæ¯ç´¢å¼• {i} ä»ç„¶ç¼ºå°‘ reasoning_contentï¼Œå¼ºåˆ¶æ·»åŠ ")
                            msg["reasoning_content"] = msg.get("content", "æ­£åœ¨æ€è€ƒå¦‚ä½•ä½¿ç”¨å·¥å…·æ¥å›ç­”è¿™ä¸ªé—®é¢˜...")
            
            try:
                return original_create(*args, **kwargs)
            except Exception as e:
                error_str = str(e)
                # If we still get reasoning_content error, try more aggressive fix
                if "reasoning_content" in error_str.lower() and "messages" in kwargs:
                    logger.warning(f"âš ï¸ [ä¸»åŠ¨åŒ…è£…] ä»ç„¶é‡åˆ° reasoning_content é”™è¯¯ï¼Œå°è¯•æ›´æ¿€è¿›çš„ä¿®å¤")
                    logger.debug(f"é”™è¯¯è¯¦æƒ…: {error_str[:300]}")
                    
                    # More aggressive fix: ensure ALL assistant messages with tool_calls have reasoning_content
                    for i, msg in enumerate(kwargs["messages"]):
                        if isinstance(msg, dict) and msg.get("role") == "assistant":
                            if msg.get("tool_calls") and "reasoning_content" not in msg:
                                msg["reasoning_content"] = msg.get("content", "æ­£åœ¨æ€è€ƒå¦‚ä½•ä½¿ç”¨å·¥å…·æ¥å›ç­”è¿™ä¸ªé—®é¢˜...")
                                logger.info(f"âœ… [ä¸»åŠ¨åŒ…è£…-é”™è¯¯ä¿®å¤-æ¶ˆæ¯ {i}] å¼ºåˆ¶æ·»åŠ  reasoning_content")
                    
                    logger.info(f"âœ… [ä¸»åŠ¨åŒ…è£…-é”™è¯¯ä¿®å¤] å·²ä¿®å¤æ‰€æœ‰æ¶ˆæ¯çš„ reasoning_content")
                    
                    # Retry with fixed messages
                    return original_create(*args, **kwargs)
                raise
        
        # Replace create method permanently at client level
        wrapped_model.client.create = proactive_wrapped_create
        
        # Also wrap chat.completions.create if it exists (more direct path)
        # This is the actual method called by LangChain ChatOpenAI
        if hasattr(wrapped_model.client, 'chat') and hasattr(wrapped_model.client.chat, 'completions'):
            original_chat_create = wrapped_model.client.chat.completions.create
            
            def wrapped_chat_create(*args, **kwargs):
                """Wrapper for chat.completions.create that adds reasoning_content.
                
                This is the direct path used by LangChain ChatOpenAI, so we need to intercept here.
                """
                if "messages" in kwargs:
                    # Log message details before processing
                    logger.debug(f"ğŸ” [chat.completions.create] å¤„ç† {len(kwargs['messages'])} æ¡æ¶ˆæ¯")
                    for i, msg in enumerate(kwargs["messages"]):
                        if isinstance(msg, dict):
                            role = msg.get("role")
                            has_tool_calls = bool(msg.get("tool_calls"))
                            has_reasoning = bool(msg.get("reasoning_content"))
                            logger.debug(f"  æ¶ˆæ¯ {i}: role={role}, tool_calls={has_tool_calls}, reasoning={has_reasoning}")
                        elif isinstance(msg, BaseMessage):
                            role = getattr(msg, 'role', getattr(msg, 'type', 'unknown'))
                            has_tool_calls = bool(getattr(msg, 'tool_calls', None))
                            has_reasoning = bool(getattr(msg, 'additional_kwargs', {}).get('reasoning_content') if hasattr(msg, 'additional_kwargs') else False)
                            logger.debug(f"  æ¶ˆæ¯ {i}: role={role}, tool_calls={has_tool_calls}, reasoning={has_reasoning}")
                    
                    # CRITICAL: Process messages to add reasoning_content
                    kwargs["messages"] = _add_reasoning_content_to_messages_helper(kwargs["messages"])
                    
                    # CRITICAL: Convert BaseMessage to dict format BEFORE API call
                    # LangChain may pass BaseMessage objects, but API needs dict format
                    processed_msgs = []
                    for i, msg in enumerate(kwargs["messages"]):
                        if isinstance(msg, BaseMessage):
                            # Convert BaseMessage to dict format
                            if isinstance(msg, AIMessage):
                                msg_dict = {
                                    "role": "assistant",
                                    "content": msg.content if hasattr(msg, 'content') else "",
                                }
                            elif isinstance(msg, ToolMessage):
                                msg_dict = {
                                    "role": "tool",
                                    "content": msg.content if hasattr(msg, 'content') else "",
                                }
                                # ToolMessage needs tool_call_id
                                if hasattr(msg, 'tool_call_id') and msg.tool_call_id:
                                    msg_dict["tool_call_id"] = msg.tool_call_id
                            else:
                                # HumanMessage or other types
                                msg_dict = {
                                    "role": "user",
                                    "content": msg.content if hasattr(msg, 'content') else "",
                                }
                            
                            # Extract tool_calls if present (for AIMessage)
                            if isinstance(msg, AIMessage):
                                if hasattr(msg, 'tool_calls') and msg.tool_calls:
                                    msg_dict["tool_calls"] = msg.tool_calls
                                elif hasattr(msg, 'additional_kwargs') and msg.additional_kwargs:
                                    tool_calls = msg.additional_kwargs.get('tool_calls')
                                    if tool_calls:
                                        msg_dict["tool_calls"] = tool_calls
                                
                                # Extract reasoning_content from additional_kwargs
                                if hasattr(msg, 'additional_kwargs') and msg.additional_kwargs:
                                    reasoning = msg.additional_kwargs.get('reasoning_content')
                                    if reasoning:
                                        msg_dict["reasoning_content"] = reasoning
                                        logger.debug(f"ğŸ” [æ¶ˆæ¯ç´¢å¼• {i}] ä» additional_kwargs æå– reasoning_content (é•¿åº¦: {len(reasoning)})")
                            
                            # CRITICAL: If tool_calls exist but no reasoning_content, add it
                            if msg_dict.get("tool_calls") and "reasoning_content" not in msg_dict:
                                reasoning = msg_dict.get("content", "")
                                if not reasoning or reasoning.strip() == "":
                                    reasoning = "æ­£åœ¨æ€è€ƒå¦‚ä½•ä½¿ç”¨å·¥å…·æ¥å›ç­”è¿™ä¸ªé—®é¢˜..."
                                msg_dict["reasoning_content"] = reasoning
                                logger.info(f"âœ… [chat.completions.create] æ¶ˆæ¯ç´¢å¼• {i} BaseMessageè½¬dictåæ·»åŠ  reasoning_content")
                            
                            processed_msgs.append(msg_dict)
                        elif isinstance(msg, dict):
                            # Already dict format, but verify reasoning_content
                            if msg.get("role") == "assistant" and msg.get("tool_calls"):
                                if "reasoning_content" not in msg:
                                    reasoning = msg.get("content", "")
                                    if not reasoning or reasoning.strip() == "":
                                        reasoning = "æ­£åœ¨æ€è€ƒå¦‚ä½•ä½¿ç”¨å·¥å…·æ¥å›ç­”è¿™ä¸ªé—®é¢˜..."
                                    msg["reasoning_content"] = reasoning
                                    logger.info(f"âœ… [chat.completions.create] æ¶ˆæ¯ç´¢å¼• {i} dictæ ¼å¼å¼ºåˆ¶æ·»åŠ  reasoning_content")
                            processed_msgs.append(msg)
                        else:
                            processed_msgs.append(msg)
                    
                    kwargs["messages"] = processed_msgs
                    
                    # Final verification: ensure all assistant messages with tool_calls have reasoning_content
                    for i, msg in enumerate(kwargs["messages"]):
                        if isinstance(msg, dict) and msg.get("role") == "assistant":
                            tool_calls = msg.get("tool_calls")
                            if tool_calls:
                                if "reasoning_content" not in msg:
                                    logger.warning(f"âš ï¸ [æœ€ç»ˆéªŒè¯å¤±è´¥] æ¶ˆæ¯ç´¢å¼• {i} ä»ç„¶ç¼ºå°‘ reasoning_contentï¼Œå¼ºåˆ¶æ·»åŠ ")
                                    reasoning = msg.get("content", "")
                                    if not reasoning or reasoning.strip() == "":
                                        reasoning = "æ­£åœ¨æ€è€ƒå¦‚ä½•ä½¿ç”¨å·¥å…·æ¥å›ç­”è¿™ä¸ªé—®é¢˜..."
                                    msg["reasoning_content"] = reasoning
                                    logger.info(f"âœ… [æœ€ç»ˆä¿®å¤] æ¶ˆæ¯ç´¢å¼• {i} å·²æ·»åŠ  reasoning_content")
                    
                    # Log final state before API call
                    logger.debug(f"ğŸ” [chat.completions.create] APIè°ƒç”¨å‰æœ€ç»ˆæ£€æŸ¥:")
                    for i, msg in enumerate(kwargs["messages"]):
                        if isinstance(msg, dict):
                            role = msg.get("role")
                            tool_calls = msg.get("tool_calls")
                            reasoning = msg.get("reasoning_content")
                            logger.debug(f"  æ¶ˆæ¯ {i}: role={role}, tool_calls={bool(tool_calls)}, reasoning={bool(reasoning)}")
                            if role == "assistant" and tool_calls and not reasoning:
                                logger.error(f"âŒ [ä¸¥é‡é”™è¯¯] æ¶ˆæ¯ç´¢å¼• {i} æœ‰ tool_calls ä½†ç¼ºå°‘ reasoning_contentï¼")
                
                try:
                    return original_chat_create(*args, **kwargs)
                except Exception as e:
                    error_str = str(e)
                    if "reasoning_content" in error_str.lower() and "messages" in kwargs:
                        logger.warning(f"âš ï¸ [chat.completions.create] é‡åˆ° reasoning_content é”™è¯¯ï¼Œå°è¯•ä¿®å¤")
                        logger.debug(f"é”™è¯¯è¯¦æƒ…: {error_str[:300]}")
                        
                        # More aggressive fix: ensure ALL assistant messages have reasoning_content
                        # Check for message index in error message to identify the problematic message
                        index_match = re.search(r'message index (\d+)', error_str.lower())
                        if index_match:
                            problem_index = int(index_match.group(1))
                            logger.info(f"ğŸ” é”™è¯¯ä¿¡æ¯æŒ‡å‡ºé—®é¢˜åœ¨æ¶ˆæ¯ç´¢å¼• {problem_index}")
                        
                        # Fix ALL assistant messages with tool_calls, not just the one mentioned
                        for i, msg in enumerate(kwargs["messages"]):
                            if isinstance(msg, dict) and msg.get("role") == "assistant":
                                if msg.get("tool_calls") and "reasoning_content" not in msg:
                                    reasoning = msg.get("content", "")
                                    if not reasoning or reasoning.strip() == "":
                                        reasoning = "æ­£åœ¨æ€è€ƒå¦‚ä½•ä½¿ç”¨å·¥å…·æ¥å›ç­”è¿™ä¸ªé—®é¢˜..."
                                    msg["reasoning_content"] = reasoning
                                    logger.info(f"âœ… [é”™è¯¯ä¿®å¤-æ¶ˆæ¯ {i}] å¼ºåˆ¶æ·»åŠ  reasoning_content")
                            elif isinstance(msg, BaseMessage) and isinstance(msg, AIMessage):
                                # Handle BaseMessage format
                                tool_calls = None
                                if hasattr(msg, 'tool_calls') and msg.tool_calls:
                                    tool_calls = msg.tool_calls
                                elif hasattr(msg, 'additional_kwargs') and msg.additional_kwargs:
                                    tool_calls = msg.additional_kwargs.get('tool_calls')
                                
                                if tool_calls:
                                    if not hasattr(msg, 'additional_kwargs') or msg.additional_kwargs is None:
                                        msg.additional_kwargs = {}
                                    if 'reasoning_content' not in msg.additional_kwargs:
                                        reasoning = msg.content if hasattr(msg, 'content') and msg.content else ""
                                        if not reasoning or reasoning.strip() == "":
                                            reasoning = "æ­£åœ¨æ€è€ƒå¦‚ä½•ä½¿ç”¨å·¥å…·æ¥å›ç­”è¿™ä¸ªé—®é¢˜..."
                                        msg.additional_kwargs['reasoning_content'] = reasoning
                                        logger.info(f"âœ… [é”™è¯¯ä¿®å¤-æ¶ˆæ¯ {i}] BaseMessage å¼ºåˆ¶æ·»åŠ  reasoning_content")
                        
                        # Re-process messages after fixing
                        kwargs["messages"] = _add_reasoning_content_to_messages_helper(kwargs["messages"])
                        
                        logger.info(f"âœ… [chat.completions.create-é”™è¯¯ä¿®å¤] å·²ä¿®å¤æ‰€æœ‰æ¶ˆæ¯çš„ reasoning_content")
                        return original_chat_create(*args, **kwargs)
                    raise
            
            wrapped_model.client.chat.completions.create = wrapped_chat_create
        
        # CRITICAL: Also wrap async_client (used by _agenerate)
        # LangChain ChatOpenAI uses self.async_client which points to root_async_client.chat.completions
        # So we need to wrap the 'create' method on async_client object directly
        if hasattr(wrapped_model, 'async_client') and hasattr(wrapped_model.async_client, 'create'):
            original_async_chat_create = wrapped_model.async_client.create
            
            async def wrapped_async_chat_create(*args, **kwargs):
                """Async wrapper for chat.completions.create that adds reasoning_content.
                
                This is the async path used by LangChain ChatOpenAI's _agenerate, so we need to intercept here.
                """
                if "messages" in kwargs:
                    # Log message details before processing
                    logger.info(f"ğŸ” [async_chat.completions.create] å¤„ç† {len(kwargs['messages'])} æ¡æ¶ˆæ¯")
                    for i, msg in enumerate(kwargs["messages"]):
                        if isinstance(msg, dict):
                            role = msg.get("role")
                            has_tool_calls = bool(msg.get("tool_calls"))
                            has_reasoning = bool(msg.get("reasoning_content"))
                            logger.info(f"  æ¶ˆæ¯ {i}: role={role}, tool_calls={has_tool_calls}, reasoning={has_reasoning}")
                        elif isinstance(msg, BaseMessage):
                            role = getattr(msg, 'role', getattr(msg, 'type', 'unknown'))
                            has_tool_calls = bool(getattr(msg, 'tool_calls', None))
                            has_reasoning = bool(getattr(msg, 'additional_kwargs', {}).get('reasoning_content') if hasattr(msg, 'additional_kwargs') else False)
                            logger.info(f"  æ¶ˆæ¯ {i}: role={role}, tool_calls={has_tool_calls}, reasoning={has_reasoning}")
                    
                    # CRITICAL: Process messages to add reasoning_content
                    kwargs["messages"] = _add_reasoning_content_to_messages_helper(kwargs["messages"])
                    
                    # CRITICAL: Convert BaseMessage to dict format BEFORE API call
                    # LangChain may pass BaseMessage objects, but API needs dict format
                    processed_msgs = []
                    for i, msg in enumerate(kwargs["messages"]):
                        if isinstance(msg, BaseMessage):
                            # Convert BaseMessage to dict format
                            if isinstance(msg, AIMessage):
                                msg_dict = {
                                    "role": "assistant",
                                    "content": msg.content if hasattr(msg, 'content') else "",
                                }
                            elif isinstance(msg, ToolMessage):
                                msg_dict = {
                                    "role": "tool",
                                    "content": msg.content if hasattr(msg, 'content') else "",
                                }
                                # ToolMessage needs tool_call_id
                                if hasattr(msg, 'tool_call_id') and msg.tool_call_id:
                                    msg_dict["tool_call_id"] = msg.tool_call_id
                            else:
                                # HumanMessage or other types
                                msg_dict = {
                                    "role": "user",
                                    "content": msg.content if hasattr(msg, 'content') else "",
                                }
                            
                            # Extract tool_calls if present (for AIMessage)
                            if isinstance(msg, AIMessage):
                                if hasattr(msg, 'tool_calls') and msg.tool_calls:
                                    msg_dict["tool_calls"] = msg.tool_calls
                                elif hasattr(msg, 'additional_kwargs') and msg.additional_kwargs:
                                    tool_calls = msg.additional_kwargs.get('tool_calls')
                                    if tool_calls:
                                        msg_dict["tool_calls"] = tool_calls
                                
                                # Extract reasoning_content from additional_kwargs
                                if hasattr(msg, 'additional_kwargs') and msg.additional_kwargs:
                                    reasoning = msg.additional_kwargs.get('reasoning_content')
                                    if reasoning:
                                        msg_dict["reasoning_content"] = reasoning
                                        logger.debug(f"ğŸ” [async-æ¶ˆæ¯ç´¢å¼• {i}] ä» additional_kwargs æå– reasoning_content (é•¿åº¦: {len(reasoning)})")
                            
                            # CRITICAL: If tool_calls exist but no reasoning_content, add it
                            if msg_dict.get("tool_calls") and "reasoning_content" not in msg_dict:
                                reasoning = msg_dict.get("content", "")
                                if not reasoning or reasoning.strip() == "":
                                    reasoning = "æ­£åœ¨æ€è€ƒå¦‚ä½•ä½¿ç”¨å·¥å…·æ¥å›ç­”è¿™ä¸ªé—®é¢˜..."
                                msg_dict["reasoning_content"] = reasoning
                                logger.info(f"âœ… [async_chat.completions.create] æ¶ˆæ¯ç´¢å¼• {i} BaseMessageè½¬dictåæ·»åŠ  reasoning_content")
                            
                            processed_msgs.append(msg_dict)
                        elif isinstance(msg, dict):
                            # Already dict format, but verify reasoning_content
                            if msg.get("role") == "assistant" and msg.get("tool_calls"):
                                if "reasoning_content" not in msg:
                                    reasoning = msg.get("content", "")
                                    if not reasoning or reasoning.strip() == "":
                                        reasoning = "æ­£åœ¨æ€è€ƒå¦‚ä½•ä½¿ç”¨å·¥å…·æ¥å›ç­”è¿™ä¸ªé—®é¢˜..."
                                    msg["reasoning_content"] = reasoning
                                    logger.info(f"âœ… [async_chat.completions.create] æ¶ˆæ¯ç´¢å¼• {i} dictæ ¼å¼å¼ºåˆ¶æ·»åŠ  reasoning_content")
                            processed_msgs.append(msg)
                        else:
                            processed_msgs.append(msg)
                    
                    kwargs["messages"] = processed_msgs
                    
                    # Final verification: ensure all assistant messages with tool_calls have reasoning_content
                    for i, msg in enumerate(kwargs["messages"]):
                        if isinstance(msg, dict) and msg.get("role") == "assistant":
                            tool_calls = msg.get("tool_calls")
                            if tool_calls:
                                if "reasoning_content" not in msg:
                                    logger.warning(f"âš ï¸ [async-æœ€ç»ˆéªŒè¯å¤±è´¥] æ¶ˆæ¯ç´¢å¼• {i} ä»ç„¶ç¼ºå°‘ reasoning_contentï¼Œå¼ºåˆ¶æ·»åŠ ")
                                    reasoning = msg.get("content", "")
                                    if not reasoning or reasoning.strip() == "":
                                        reasoning = "æ­£åœ¨æ€è€ƒå¦‚ä½•ä½¿ç”¨å·¥å…·æ¥å›ç­”è¿™ä¸ªé—®é¢˜..."
                                    msg["reasoning_content"] = reasoning
                                    logger.info(f"âœ… [async-æœ€ç»ˆä¿®å¤] æ¶ˆæ¯ç´¢å¼• {i} å·²æ·»åŠ  reasoning_content")
                    
                    # Log final state before API call
                    logger.info(f"ğŸ” [async_chat.completions.create] APIè°ƒç”¨å‰æœ€ç»ˆæ£€æŸ¥:")
                    for i, msg in enumerate(kwargs["messages"]):
                        if isinstance(msg, dict):
                            role = msg.get("role")
                            tool_calls = msg.get("tool_calls")
                            reasoning = msg.get("reasoning_content")
                            logger.info(f"  æ¶ˆæ¯ {i}: role={role}, tool_calls={bool(tool_calls)}, reasoning={bool(reasoning)}")
                            if role == "assistant" and tool_calls and not reasoning:
                                logger.error(f"âŒ [ä¸¥é‡é”™è¯¯] æ¶ˆæ¯ç´¢å¼• {i} æœ‰ tool_calls ä½†ç¼ºå°‘ reasoning_contentï¼")
                
                try:
                    return await original_async_chat_create(*args, **kwargs)
                except Exception as e:
                    error_str = str(e)
                    if "reasoning_content" in error_str.lower() and "messages" in kwargs:
                        logger.warning(f"âš ï¸ [async_chat.completions.create] é‡åˆ° reasoning_content é”™è¯¯ï¼Œå°è¯•ä¿®å¤")
                        logger.debug(f"é”™è¯¯è¯¦æƒ…: {error_str[:300]}")
                        
                        # More aggressive fix: ensure ALL assistant messages have reasoning_content
                        # Check for message index in error message to identify the problematic message
                        index_match = re.search(r'message index (\d+)', error_str.lower())
                        if index_match:
                            problem_index = int(index_match.group(1))
                            logger.info(f"ğŸ” é”™è¯¯ä¿¡æ¯æŒ‡å‡ºé—®é¢˜åœ¨æ¶ˆæ¯ç´¢å¼• {problem_index}")
                        
                        # Fix ALL assistant messages with tool_calls, not just the one mentioned
                        for i, msg in enumerate(kwargs["messages"]):
                            if isinstance(msg, dict) and msg.get("role") == "assistant":
                                if msg.get("tool_calls") and "reasoning_content" not in msg:
                                    reasoning = msg.get("content", "")
                                    if not reasoning or reasoning.strip() == "":
                                        reasoning = "æ­£åœ¨æ€è€ƒå¦‚ä½•ä½¿ç”¨å·¥å…·æ¥å›ç­”è¿™ä¸ªé—®é¢˜..."
                                    msg["reasoning_content"] = reasoning
                                    logger.info(f"âœ… [async-é”™è¯¯ä¿®å¤-æ¶ˆæ¯ {i}] å¼ºåˆ¶æ·»åŠ  reasoning_content")
                            elif isinstance(msg, BaseMessage) and isinstance(msg, AIMessage):
                                # Handle BaseMessage format
                                tool_calls = None
                                if hasattr(msg, 'tool_calls') and msg.tool_calls:
                                    tool_calls = msg.tool_calls
                                elif hasattr(msg, 'additional_kwargs') and msg.additional_kwargs:
                                    tool_calls = msg.additional_kwargs.get('tool_calls')
                                
                                if tool_calls:
                                    if not hasattr(msg, 'additional_kwargs') or msg.additional_kwargs is None:
                                        msg.additional_kwargs = {}
                                    if 'reasoning_content' not in msg.additional_kwargs:
                                        reasoning = msg.content if hasattr(msg, 'content') and msg.content else ""
                                        if not reasoning or reasoning.strip() == "":
                                            reasoning = "æ­£åœ¨æ€è€ƒå¦‚ä½•ä½¿ç”¨å·¥å…·æ¥å›ç­”è¿™ä¸ªé—®é¢˜..."
                                        msg.additional_kwargs['reasoning_content'] = reasoning
                                        logger.info(f"âœ… [async-é”™è¯¯ä¿®å¤-æ¶ˆæ¯ {i}] BaseMessage å¼ºåˆ¶æ·»åŠ  reasoning_content")
                        
                        # Re-process messages after fixing
                        kwargs["messages"] = _add_reasoning_content_to_messages_helper(kwargs["messages"])
                        
                        logger.info(f"âœ… [async_chat.completions.create-é”™è¯¯ä¿®å¤] å·²ä¿®å¤æ‰€æœ‰æ¶ˆæ¯çš„ reasoning_content")
                        return await original_async_chat_create(*args, **kwargs)
                    raise
            
            wrapped_model.async_client.create = wrapped_async_chat_create
            logger.info("âœ… å·²åŒ…è£… async_client.create æ–¹æ³•")
        
        # Also wrap stream method if it exists (for streaming)
        if hasattr(wrapped_model.client, 'stream'):
            original_stream = wrapped_model.client.stream
            
            def add_reasoning_to_messages(messages):
                """Helper to add reasoning_content for stream method.
                
                Uses the standalone helper function to ensure consistency across all code paths.
                """
                return _add_reasoning_content_to_messages_helper(messages)
            
            def wrapped_stream(*args, **kwargs):
                """Wrapper for stream method that adds reasoning_content."""
                if "messages" in kwargs:
                    kwargs["messages"] = add_reasoning_to_messages(kwargs["messages"])
                return original_stream(*args, **kwargs)
            
            wrapped_model.client.stream = wrapped_stream
        
        return wrapped_model

