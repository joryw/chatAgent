"""ReAct Agent implementation using LangChain."""

import asyncio
import logging
import time
from typing import Optional, AsyncIterator, Any

from langgraph.prebuilt import create_react_agent
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.agents import AgentAction, AgentFinish
from langchain_core.callbacks import AsyncCallbackHandler
from langchain_core.language_models import BaseChatModel
from langchain_core.tools import BaseTool
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

from src.agents.base import (
    BaseAgent,
    AgentStep,
    AgentResult,
    AgentTimeoutError,
    AgentIterationLimitError,
    AgentExecutionError,
)
from src.agents.tools.search_tool import SearchTool
from src.config.agent_config import AgentConfig
from src.config.langsmith_config import get_langsmith_tracer

logger = logging.getLogger(__name__)


# ReAct Prompt Template (ä¸­æ–‡ç‰ˆ)
REACT_PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„ AI åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·æ¥å¸®åŠ©å›žç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ä½ æœ‰æƒè®¿é—®ä»¥ä¸‹å·¥å…·:

{tools}

ä½¿ç”¨ä»¥ä¸‹æ ¼å¼è¿›è¡ŒæŽ¨ç†å’Œè¡ŒåŠ¨:

Question: ç”¨æˆ·çš„è¾“å…¥é—®é¢˜
Thought: ä½ åº”è¯¥æ€è€ƒå¦‚ä½•å›žç­”è¿™ä¸ªé—®é¢˜
Action: è¦ä½¿ç”¨çš„å·¥å…·åç§°ï¼Œåº”è¯¥æ˜¯ [{tool_names}] ä¹‹ä¸€
Action Input: å·¥å…·çš„è¾“å…¥å‚æ•°
Observation: å·¥å…·è¿”å›žçš„ç»“æžœ
... (è¿™ä¸ª Thought/Action/Action Input/Observation å¯ä»¥é‡å¤ N æ¬¡)
Thought: æˆ‘çŽ°åœ¨çŸ¥é“æœ€ç»ˆç­”æ¡ˆäº†
Final Answer: å¯¹ç”¨æˆ·é—®é¢˜çš„æœ€ç»ˆå›žç­”

é‡è¦è§„åˆ™:
1. å½“éœ€è¦æœ€æ–°ä¿¡æ¯æˆ–å®žæ—¶æ•°æ®æ—¶ï¼Œä½¿ç”¨ web_search å·¥å…·
2. å½“å·¥å…·è¿”å›žç»“æžœåŽï¼Œä»”ç»†åˆ†æžæ˜¯å¦è¶³å¤Ÿå›žç­”é—®é¢˜
3. å¦‚æžœä¿¡æ¯ä¸è¶³ï¼Œå¯ä»¥å†æ¬¡ä½¿ç”¨å·¥å…·æœç´¢æ›´å¤šä¿¡æ¯
4. åœ¨æœ€ç»ˆå›žç­”ä¸­ï¼Œä½¿ç”¨ [æ•°å­—] æ ¼å¼å¼•ç”¨æœç´¢ç»“æžœçš„æ¥æº
5. æœ€ç»ˆå›žç­”åº”è¯¥å‡†ç¡®ã€å®Œæ•´ã€æœ‰å¼•ç”¨

å¼€å§‹!

Question: {input}
Thought: {agent_scratchpad}
"""


class StreamingCallbackHandler(AsyncCallbackHandler):
    """Callback handler for streaming Agent steps."""
    
    def __init__(self, step_queue: asyncio.Queue):
        """Initialize callback handler.
        
        Args:
            step_queue: Queue to put AgentStep objects
        """
        self.step_queue = step_queue
        self.current_reasoning = ""
    
    async def on_agent_action(
        self, action: AgentAction, **kwargs: Any
    ) -> None:
        """Called when agent takes an action.
        
        Args:
            action: AgentAction object
        """
        # Send reasoning step if we have accumulated text
        if self.current_reasoning:
            await self.step_queue.put(
                AgentStep(
                    type="reasoning",
                    content=self.current_reasoning.strip(),
                )
            )
            self.current_reasoning = ""
        
        # Send action step
        await self.step_queue.put(
            AgentStep(
                type="action",
                content=f"ä½¿ç”¨å·¥å…·: {action.tool}",
                metadata={
                    "tool": action.tool,
                    "tool_input": action.tool_input,
                }
            )
        )
    
    async def on_agent_finish(
        self, finish: AgentFinish, **kwargs: Any
    ) -> None:
        """Called when agent finishes.
        
        Args:
            finish: AgentFinish object
        """
        # Send final reasoning if any
        if self.current_reasoning:
            await self.step_queue.put(
                AgentStep(
                    type="reasoning",
                    content=self.current_reasoning.strip(),
                )
            )
        
        # Send final answer
        await self.step_queue.put(
            AgentStep(
                type="final",
                content=finish.return_values.get("output", ""),
            )
        )
    
    async def on_tool_start(
        self, serialized: dict[str, Any], input_str: str, **kwargs: Any
    ) -> None:
        """Called when tool starts.
        
        Args:
            serialized: Tool serialization
            input_str: Tool input string
        """
        tool_name = serialized.get("name", "unknown")
        logger.info(f"ðŸ”§ å·¥å…·å¼€å§‹: {tool_name}, è¾“å…¥: {input_str}")
    
    async def on_tool_end(self, output: str, **kwargs: Any) -> None:
        """Called when tool ends.
        
        Args:
            output: Tool output string
        """
        # Send observation step
        await self.step_queue.put(
            AgentStep(
                type="observation",
                content=output,
            )
        )
    
    async def on_tool_error(self, error: Exception, **kwargs: Any) -> None:
        """Called when tool encounters an error.
        
        Args:
            error: Exception that occurred
        """
        logger.error(f"âŒ å·¥å…·æ‰§è¡Œé”™è¯¯: {error}")
        await self.step_queue.put(
            AgentStep(
                type="observation",
                content=f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(error)}",
            )
        )
    
    async def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """Called when LLM generates a new token.
        
        Args:
            token: New token string
        """
        # Accumulate reasoning tokens
        self.current_reasoning += token


class ReActAgent(BaseAgent):
    """ReAct Agent implementation.
    
    This agent implements the ReAct (Reasoning + Acting) pattern using LangChain.
    It can use tools (like web search) to gather information before answering.
    
    Supports dual LLM architecture:
    - function_call_llm: Used for tool calling decisions
    - answer_llm: Used for generating final answers
    
    Attributes:
        config: Agent configuration
        function_call_llm: Language model for function calling
        answer_llm: Language model for answer generation
        tools: List of available tools
        agent_executor: LangChain AgentExecutor (uses function_call_llm)
    """
    
    def __init__(
        self,
        llm: BaseChatModel,
        search_tool: SearchTool,
        config: Optional[AgentConfig] = None,
        answer_llm: Optional[BaseChatModel] = None,
    ):
        """Initialize ReAct Agent.
        
        Args:
            llm: Language model instance for function calling (function_call_llm)
            search_tool: Search tool instance
            config: Agent configuration (optional)
            answer_llm: Optional language model for answer generation.
                       If None, uses llm for both stages (backward compatibility)
        """
        self.config = config or AgentConfig()
        self.function_call_llm = llm
        # If answer_llm is not provided, use function_call_llm for both stages
        self.answer_llm = answer_llm if answer_llm is not None else llm
        self.tools = [search_tool]
        
        # CRITICAL: Bind tools to the model for function calling
        # LangGraph's create_react_agent requires the model to have tools bound
        # Some models (like DeepSeek) need explicit tool binding
        try:
            # Try to bind tools if the model supports it
            if hasattr(self.function_call_llm, 'bind_tools'):
                logger.info(f"ðŸ”§ ç»‘å®š {len(self.tools)} ä¸ªå·¥å…·åˆ°æ¨¡åž‹...")
                bound_llm = self.function_call_llm.bind_tools(self.tools)
            else:
                # If bind_tools is not available, use the model as-is
                # LangGraph's create_react_agent should handle tool binding internally
                logger.debug("æ¨¡åž‹ä¸æ”¯æŒ bind_toolsï¼Œä½¿ç”¨åŽŸå§‹æ¨¡åž‹")
                bound_llm = self.function_call_llm
        except Exception as e:
            logger.warning(f"âš ï¸ å·¥å…·ç»‘å®šå¤±è´¥ï¼Œä½¿ç”¨åŽŸå§‹æ¨¡åž‹: {e}")
            bound_llm = self.function_call_llm
        
        # Create ReAct agent using LangGraph with function_call_llm
        # LangGraph's create_react_agent handles prompt creation internally
        # It will automatically bind tools if not already bound
        self.agent_executor = create_react_agent(
            model=bound_llm,
            tools=self.tools,
        )
        
        logger.info(f"âœ… Agent executor åˆ›å»ºå®Œæˆï¼Œå·¥å…·æ•°é‡: {len(self.tools)}")
        
        # Track if using dual LLM mode
        using_dual_llm = answer_llm is not None
        
        # Log tool information for debugging
        tool_names = [tool.name for tool in self.tools]
        logger.info(
            f"âœ… ReAct Agent åˆå§‹åŒ–å®Œæˆ "
            f"(max_iterations={self.config.max_iterations}, "
            f"max_execution_time={self.config.max_execution_time}s, "
            f"dual_llm_mode={using_dual_llm}, "
            f"tools={tool_names})"
        )
    
    def _should_generate_answer(self, tool_results: list[str], iteration_count: int) -> bool:
        """Evaluate if tool calling results are sufficient to generate answer.
        
        Args:
            tool_results: List of tool execution results
            iteration_count: Number of tool calling iterations performed
            
        Returns:
            True if should generate answer, False if should continue tool calling
        """
        # Stop if reached max iterations
        if iteration_count >= self.config.max_iterations:
            logger.info(f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({self.config.max_iterations})ï¼Œåœæ­¢å·¥å…·è°ƒç”¨")
            return True
        
        # Stop if we have results and they're not empty
        if tool_results:
            # Check if any result has meaningful content (more than 50 chars)
            meaningful_results = [r for r in tool_results if len(r.strip()) > 50]
            if meaningful_results:
                logger.info(f"å·¥å…·è°ƒç”¨ç»“æžœå……è¶³ ({len(meaningful_results)} æ¡æœ‰æ•ˆç»“æžœ)ï¼Œå‡†å¤‡ç”Ÿæˆå›žç­”")
                return True
        
        # Continue tool calling
        return False
    
    async def _generate_answer_with_answer_llm(
        self, 
        user_input: str, 
        tool_results: list[str],
        tool_calls: list[dict]
    ) -> str:
        """Generate final answer using answer_llm based on tool results.
        
        Args:
            user_input: Original user question
            tool_results: List of tool execution results
            tool_calls: List of tool call information
            
        Returns:
            Generated final answer
        """
        # Build context from tool results
        context_parts = []
        for i, result in enumerate(tool_results, 1):
            context_parts.append(f"[æœç´¢ç»“æžœ {i}]\n{result}")
        
        context = "\n\n".join(context_parts)
        
        # Build prompt for answer generation
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„ AI åŠ©æ‰‹ã€‚åŸºäºŽä»¥ä¸‹æœç´¢ç»“æžœï¼Œä¸ºç”¨æˆ·çš„é—®é¢˜æä¾›ä¸€ä¸ªå‡†ç¡®ã€å®Œæ•´ã€æœ‰å¼•ç”¨çš„å›žç­”ã€‚

é‡è¦è§„åˆ™:
1. ä»”ç»†åˆ†æžæœç´¢ç»“æžœï¼Œæå–ç›¸å…³ä¿¡æ¯
2. åœ¨å›žç­”ä¸­ä½¿ç”¨ [æ•°å­—] æ ¼å¼å¼•ç”¨æœç´¢ç»“æžœæ¥æº
3. å¦‚æžœæœç´¢ç»“æžœä¸è¶³ä»¥å›žç­”é—®é¢˜ï¼Œå¦‚å®žè¯´æ˜Ž
4. å›žç­”åº”è¯¥å‡†ç¡®ã€å®Œæ•´ã€æœ‰æ¡ç†
"""
        
        user_prompt = f"""ç”¨æˆ·é—®é¢˜: {user_input}

æœç´¢ç»“æžœ:
{context}

è¯·åŸºäºŽä»¥ä¸Šæœç´¢ç»“æžœå›žç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""
        
        # Generate answer using answer_llm
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        logger.info(f"ä½¿ç”¨ answer_llm ç”Ÿæˆæœ€ç»ˆå›žç­”...")
        response = await self.answer_llm.ainvoke(messages)
        answer = response.content if hasattr(response, 'content') else str(response)
        
        return answer
    
    async def run(self, user_input: str) -> AgentResult:
        """Run agent on user input.
        
        Args:
            user_input: User's question
            
        Returns:
            AgentResult with final answer and steps
            
        Raises:
            AgentTimeoutError: If execution exceeds time limit
            AgentExecutionError: If execution fails
        """
        logger.info(f"ðŸ¤– Agent å¼€å§‹æ‰§è¡Œ: {user_input}")
        start_time = time.time()
        
        # Check if using dual LLM mode
        using_dual_llm = self.answer_llm is not self.function_call_llm
        
        try:
            # Get LangSmith tracer if enabled
            tracer = get_langsmith_tracer()
            callbacks = [tracer] if tracer else None
            
            # Run agent with timeout using LangGraph API
            # LangGraph expects messages, not a dict with "input" key
            invoke_input = {"messages": [HumanMessage(content=user_input)]}
            
            # Prepare config with callbacks if LangSmith is enabled
            if callbacks:
                invoke_config = {"callbacks": callbacks}
                result = await asyncio.wait_for(
                    self.agent_executor.ainvoke(invoke_input, config=invoke_config),
                    timeout=self.config.max_execution_time,
                )
            else:
                result = await asyncio.wait_for(
                    self.agent_executor.ainvoke(invoke_input),
                    timeout=self.config.max_execution_time,
                )
            
            elapsed_time = time.time() - start_time
            logger.info(f"âœ… Agent å·¥å…·è°ƒç”¨é˜¶æ®µå®Œæˆï¼Œè€—æ—¶ {elapsed_time:.2f}s")
            
            # LangGraph result contains messages
            messages = result.get("messages", [])
            
            # Extract tool results and tool calls
            tool_results = []
            tool_calls = []
            steps = []
            iteration_count = 0
            
            for msg in messages:
                if isinstance(msg, AIMessage):
                    if msg.tool_calls:
                        iteration_count += 1
                        for tool_call in msg.tool_calls:
                            tool_calls.append({
                                "name": tool_call.get("name"),
                                "args": tool_call.get("args"),
                            })
                            steps.append(
                                AgentStep(
                                    type="action",
                                    content=f"ä½¿ç”¨å·¥å…·: {tool_call.get('name', 'unknown')}",
                                    metadata={
                                        "tool": tool_call.get("name"),
                                        "tool_input": tool_call.get("args"),
                                    }
                                )
                            )
                    elif msg.content:
                        # This might be reasoning or final answer from function_call_llm
                        if not using_dual_llm:
                            # Single LLM mode: use this as final answer
                            steps.append(
                                AgentStep(
                                    type="final",
                                    content=msg.content,
                                )
                            )
                elif hasattr(msg, "content") and str(msg.content):
                    # Tool message (observation)
                    tool_results.append(str(msg.content))
                    steps.append(
                        AgentStep(
                            type="observation",
                            content=str(msg.content),
                        )
                    )
            
            # Generate final answer
            if using_dual_llm:
                # Dual LLM mode: use answer_llm to generate final answer
                logger.info("ðŸ”„ åˆ‡æ¢åˆ° answer_llm ç”Ÿæˆæœ€ç»ˆå›žç­”...")
                final_answer = await self._generate_answer_with_answer_llm(
                    user_input, tool_results, tool_calls
                )
                steps.append(
                    AgentStep(
                        type="final",
                        content=final_answer,
                    )
                )
            else:
                # Single LLM mode: use the answer from agent_executor
                final_message = messages[-1] if messages else None
                final_answer = final_message.content if final_message else ""
                if not final_answer:
                    # Fallback: generate answer if agent didn't produce one
                    logger.warning("âš ï¸ Agent æœªç”Ÿæˆæœ€ç»ˆå›žç­”ï¼Œä½¿ç”¨ answer_llm ç”Ÿæˆ...")
                    final_answer = await self._generate_answer_with_answer_llm(
                        user_input, tool_results, tool_calls
                    )
                    steps.append(
                        AgentStep(
                            type="final",
                            content=final_answer,
                        )
                    )
            
            total_time = time.time() - start_time
            logger.info(f"âœ… Agent æ‰§è¡Œå®Œæˆï¼Œæ€»è€—æ—¶ {total_time:.2f}s")
            
            return AgentResult(
                final_answer=final_answer,
                steps=steps,
                total_iterations=iteration_count,
            )
            
        except asyncio.TimeoutError:
            elapsed_time = time.time() - start_time
            logger.error(f"â±ï¸ Agent æ‰§è¡Œè¶…æ—¶ ({elapsed_time:.2f}s)")
            raise AgentTimeoutError(
                f"Agent æ‰§è¡Œè¶…æ—¶ ({self.config.max_execution_time}ç§’)ã€‚"
                f"è¯·å°è¯•ç®€åŒ–é—®é¢˜æˆ–åˆ‡æ¢åˆ° Chat æ¨¡å¼ã€‚"
            )
        
        except Exception as e:
            elapsed_time = time.time() - start_time
            logger.error(f"âŒ Agent æ‰§è¡Œå¤±è´¥ ({elapsed_time:.2f}s): {e}")
            raise AgentExecutionError(f"Agent æ‰§è¡Œå¤±è´¥: {str(e)}")
    
    async def stream(self, user_input: str) -> AsyncIterator[AgentStep]:
        """Stream agent execution steps.
        
        Args:
            user_input: User's question
            
        Yields:
            AgentStep objects as they are generated
            
        Raises:
            AgentTimeoutError: If execution exceeds time limit
        """
        logger.info(f"ðŸ¤– Agent å¼€å§‹æµå¼æ‰§è¡Œ: {user_input}")
        
        # Check if using dual LLM mode
        using_dual_llm = self.answer_llm is not self.function_call_llm
        
        try:
            # Try streaming first
            has_yielded = False
            all_messages = []
            tool_results = []
            tool_calls = []
            final_answer_from_function_call = None
            
            # Get LangSmith tracer if enabled
            tracer = get_langsmith_tracer()
            callbacks = [tracer] if tracer else None
            
            # Stream events from LangGraph
            stream_input = {"messages": [HumanMessage(content=user_input)]}
            
            # Prepare config with callbacks if LangSmith is enabled
            if callbacks:
                stream_config = {"callbacks": callbacks}
                event_stream = self.agent_executor.astream(stream_input, config=stream_config)
            else:
                event_stream = self.agent_executor.astream(stream_input)
            
            async for event in event_stream:
                has_yielded = True
                # LangGraph returns events with node names as keys
                # e.g., {"agent": {...}, "tools": {...}}
                event_keys = list(event.keys())
                logger.debug(f"æ”¶åˆ°äº‹ä»¶: {event_keys}")
                
                # Log if we're receiving agent events but no tool calls
                if "agent" in event and "tools" not in event_keys:
                    logger.debug(f"ðŸ” Agent èŠ‚ç‚¹äº‹ä»¶ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨...")
                
                # Check for agent node (thinking/reasoning)
                if "agent" in event:
                    agent_data = event["agent"]
                    if isinstance(agent_data, dict) and "messages" in agent_data:
                        messages = agent_data["messages"]
                        all_messages.extend(messages)
                        
                        # Check for reasoning (AI message without tool calls)
                        for msg in messages:
                            if isinstance(msg, AIMessage):
                                # Check for tool_calls in multiple possible locations
                                msg_tool_calls = None
                                if hasattr(msg, "tool_calls") and msg.tool_calls:
                                    msg_tool_calls = msg.tool_calls
                                elif hasattr(msg, "additional_kwargs") and msg.additional_kwargs:
                                    msg_tool_calls = msg.additional_kwargs.get("tool_calls")
                                
                                if msg_tool_calls:
                                    # This is a tool call decision
                                    logger.info(f"ðŸ”§ Agent å†³å®šè°ƒç”¨å·¥å…·ï¼Œå·¥å…·è°ƒç”¨æ•°é‡: {len(msg_tool_calls)}")
                                    for tool_call in msg_tool_calls:
                                        # Handle different tool_call formats
                                        if isinstance(tool_call, dict):
                                            tool_name = tool_call.get("name") or tool_call.get("function", {}).get("name", "unknown")
                                            tool_input = tool_call.get("args") or tool_call.get("function", {}).get("arguments", {})
                                        else:
                                            # Handle object format
                                            tool_name = getattr(tool_call, "name", "unknown")
                                            tool_input = getattr(tool_call, "args", {})
                                        
                                        tool_calls.append({
                                            "name": tool_name,
                                            "args": tool_input,
                                        })
                                        logger.info(f"ðŸ”§ Agent å†³å®šè°ƒç”¨å·¥å…·: {tool_name}, è¾“å…¥: {tool_input}")
                                        yield AgentStep(
                                            type="action",
                                            content=f"è°ƒç”¨å·¥å…·: {tool_name}",
                                            metadata={
                                                "tool": tool_name,
                                                "tool_input": str(tool_input),
                                            }
                                        )
                                else:
                                    # This is reasoning or final answer from function_call_llm
                                    content = msg.content
                                    if content and content.strip():
                                        if using_dual_llm:
                                            # In dual LLM mode, this is just reasoning
                                            # We'll generate final answer later
                                            yield AgentStep(
                                                type="reasoning",
                                                content=content[:300] + "..." if len(content) > 300 else content,
                                            )
                                        else:
                                            # In single LLM mode, this might be final answer
                                            if len(all_messages) > 1:
                                                final_answer_from_function_call = content
                                            else:
                                                yield AgentStep(
                                                    type="reasoning",
                                                    content=content[:300] + "..." if len(content) > 300 else content,
                                                )
                
                # Check for tools node (tool execution results)
                elif "tools" in event:
                    tools_data = event["tools"]
                    if isinstance(tools_data, dict) and "messages" in tools_data:
                        tool_messages = tools_data["messages"]
                        all_messages.extend(tool_messages)
                        
                        # Extract tool output
                        for msg in tool_messages:
                            if hasattr(msg, "content"):
                                tool_output = str(msg.content)
                                tool_results.append(tool_output)
                                logger.info(f"âœ… å·¥å…·æ‰§è¡Œå®Œæˆï¼Œç»“æžœé•¿åº¦: {len(tool_output)}")
                                yield AgentStep(
                                    type="observation",
                                    content=tool_output[:500] + "..." if len(tool_output) > 500 else tool_output,
                                )
            
            # Generate final answer
            if using_dual_llm:
                # Dual LLM mode: use answer_llm to generate final answer
                logger.info("ðŸ”„ åˆ‡æ¢åˆ° answer_llm ç”Ÿæˆæœ€ç»ˆå›žç­”...")
                yield AgentStep(
                    type="reasoning",
                    content="æ­£åœ¨ä½¿ç”¨ answer_llm ç”Ÿæˆæœ€ç»ˆå›žç­”...",
                )
                
                # Stream answer generation
                system_prompt = """ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„ AI åŠ©æ‰‹ã€‚åŸºäºŽä»¥ä¸‹æœç´¢ç»“æžœï¼Œä¸ºç”¨æˆ·çš„é—®é¢˜æä¾›ä¸€ä¸ªå‡†ç¡®ã€å®Œæ•´ã€æœ‰å¼•ç”¨çš„å›žç­”ã€‚

é‡è¦è§„åˆ™:
1. ä»”ç»†åˆ†æžæœç´¢ç»“æžœï¼Œæå–ç›¸å…³ä¿¡æ¯
2. åœ¨å›žç­”ä¸­ä½¿ç”¨ [æ•°å­—] æ ¼å¼å¼•ç”¨æœç´¢ç»“æžœæ¥æº
3. å¦‚æžœæœç´¢ç»“æžœä¸è¶³ä»¥å›žç­”é—®é¢˜ï¼Œå¦‚å®žè¯´æ˜Ž
4. å›žç­”åº”è¯¥å‡†ç¡®ã€å®Œæ•´ã€æœ‰æ¡ç†
"""
                
                context_parts = []
                for i, result in enumerate(tool_results, 1):
                    context_parts.append(f"[æœç´¢ç»“æžœ {i}]\n{result}")
                context = "\n\n".join(context_parts)
                
                user_prompt = f"""ç”¨æˆ·é—®é¢˜: {user_input}

æœç´¢ç»“æžœ:
{context}

è¯·åŸºäºŽä»¥ä¸Šæœç´¢ç»“æžœå›žç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""
                
                messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=user_prompt)
                ]
                
                # Stream answer generation
                async for chunk in self.answer_llm.astream(messages):
                    if hasattr(chunk, 'content') and chunk.content:
                        yield AgentStep(
                            type="final",
                            content=chunk.content,
                        )
            else:
                # Single LLM mode: use answer from function_call_llm
                if not final_answer_from_function_call:
                    # Extract final answer from all messages
                    for msg in reversed(all_messages):
                        if isinstance(msg, AIMessage):
                            if not (hasattr(msg, "tool_calls") and msg.tool_calls):
                                final_answer_from_function_call = msg.content
                                break
                
                if final_answer_from_function_call:
                    logger.info("âœ… Agent ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
                    yield AgentStep(
                        type="final",
                        content=final_answer_from_function_call,
                    )
                elif not has_yielded:
                    # Fallback: if streaming didn't work, use non-streaming method
                    logger.warning("âš ï¸ æµå¼è¾“å‡ºæœªè¿”å›žäº‹ä»¶ï¼Œä½¿ç”¨å›žé€€æ–¹æ³•")
                    yield AgentStep(
                        type="reasoning",
                        content="æ­£åœ¨å¤„ç†è¯·æ±‚...",
                    )
                    result = await self.run(user_input)
                    for step in result.steps:
                        yield step
                else:
                    logger.warning("âš ï¸ Agent æœªç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
                    yield AgentStep(
                        type="error",
                        content="Agent æœªèƒ½ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼Œè¯·é‡è¯•ã€‚",
                    )
            
            logger.info("âœ… Agent æµå¼æ‰§è¡Œå®Œæˆ")
            
        except asyncio.TimeoutError:
            logger.error(f"â±ï¸ Agent æµå¼æ‰§è¡Œè¶…æ—¶")
            yield AgentStep(
                type="error",
                content=f"æ‰§è¡Œè¶…æ—¶ ({self.config.max_execution_time}ç§’)",
            )
        except Exception as e:
            logger.error(f"âŒ Agent æµå¼æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            # Try fallback method
            try:
                logger.info("å°è¯•ä½¿ç”¨å›žé€€æ–¹æ³•...")
                yield AgentStep(
                    type="reasoning",
                    content="æµå¼è¾“å‡ºé‡åˆ°é—®é¢˜ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•å¤„ç†...",
                )
                result = await self.run(user_input)
                for step in result.steps:
                    yield step
                yield AgentStep(
                    type="final",
                    content=result.final_answer,
                )
            except Exception as fallback_error:
                logger.error(f"å›žé€€æ–¹æ³•ä¹Ÿå¤±è´¥: {fallback_error}")
                yield AgentStep(
                    type="error",
                    content=f"æ‰§è¡Œå¤±è´¥: {str(e)}",
                )
    
    def reset(self) -> None:
        """Reset agent state."""
        logger.info("ðŸ”„ é‡ç½® Agent çŠ¶æ€")
        # ReAct agent is stateless, nothing to reset
        pass
    
    def _convert_messages_to_steps(self, messages: list) -> list[AgentStep]:
        """Convert LangGraph messages to AgentStep objects.
        
        Args:
            messages: List of messages from LangGraph
            
        Returns:
            List of AgentStep objects
        """
        steps = []
        
        for msg in messages:
            # Skip human messages (user input)
            if isinstance(msg, HumanMessage):
                continue
            
            # AI messages are responses
            if isinstance(msg, AIMessage):
                if msg.tool_calls:
                    # This is a tool call
                    for tool_call in msg.tool_calls:
                        steps.append(
                            AgentStep(
                                type="action",
                                content=f"ä½¿ç”¨å·¥å…·: {tool_call.get('name', 'unknown')}",
                                metadata={
                                    "tool": tool_call.get("name"),
                                    "tool_input": tool_call.get("args"),
                                }
                            )
                        )
                else:
                    # Final answer
                    steps.append(
                        AgentStep(
                            type="final",
                            content=msg.content,
                        )
                    )
            
            # Tool messages are observations
            else:
                steps.append(
                    AgentStep(
                        type="observation",
                        content=str(msg.content),
                    )
                )
        
        return steps

